\documentclass{report}

\input{../../latex_template/preamble}
\input{../../latex_template/macros}
\input{../../latex_template/letterfonts}

\title{\Huge{MATH2001}\\Calculus \& Linear Algebra II}
\author{\huge{Michael Kasumagic, s4430266}}
\date{\huge{Summer Semester, 2024-25}}

% For the Gram-Schmidt Algo
\newcommand{\Step}[2]{\SetKwBlock{SecretStep}{Step #1}{EndStep} \SecretStep{#2}}

\begin{document}

\maketitle
\newpage% or \cleardoublepage
% \pdfbookmark[<level>]{<title>}{<dest>}
\pdfbookmark[section]{\contentsname}{toc}
\tableofcontents
\pagebreak

\chapter{Week 1}
\section{Lecture 1}
In this course we will cover four major topics:
\begin{itemize}
  \item Ordinary Differential Equations
  \item Linear Algebra
  \item Vector Calculus
  \item Integral Calculus
\end{itemize}

\subsection{Solutions to First Order ODEs}
We are comfortable solving three types of first order ODEs by now:
\begin{itemize}
  \item Directly integrable: $\dps{\dydx = f(x)}$
  $$
    y(x) = \int f(x) \d x = F(x) + C
  $$
  \item Seperable: $\dps{\dydx = f(x)g(y)}$
  $$
    \frac{1}{g(y)}\dydx = f(x) \iff \int \frac{1}{g(y)}\frac{\d y}{\cancel{\d x}}\cancel{\d x} = \int f(x) \d x \iff G(y(x)) = F(x) + C
  $$
  $$
    \text{If } G \text{ is invertible, then } y(x) = G\inv(F(x)+C) 
  $$
  \item Linear: $\dps{\dydx = q(x) - p(x)y}$
  $$
    \Let\mu = \exp\bracks{\int p(x)\d x}\implies \mu\dydx + \mu p(x)y = \mu q(x) \iff \ddx\bracks{\mu y} = \mu q(x) \iff y(x) = \frac{1}{\mu(x)}\int \mu q(x)\d x
  $$
\end{itemize}

In many applications, we need to solve an IVP. In general this is an equation of form,
$$
  \dydx = f(x,y),\quad y(x_0) = y_0.
$$
In other words, we seek to find solutions to the ODE which pass through the point $(x_0, y_0)$ in the $x{-}y$ plane.

\ex{}{
  $$
    \dydx = x,\quad y(0)=1\text{ has a unique solution:}
  $$
  \begin{align*}
    \dydx &= x \\
    y(x) &= \frac{1}{2}x^2 + C \\
    \text{Impose } y(0) &= 1 \\ 
    \tf 1 &= \frac{1}{2}(0)^2 + C \\
    \tf C &= 1 \\
    \tf y(x) &= \frac{1}{2}x^2 + 1 
  \end{align*}
}

\ex{}{
  $$
    \dydx = 3xy^{1/3},\quad y(0)=0\text{ has more than one solution:}
  $$
  \begin{align*}
    y^{-1/3}\dydx &= 3x \\
    \int y^{-1/3}\dydx\d x &= \int 3x\d x \\
    \int y^{-1/3}\d y &= \int 3x\d x \\
    \frac{3}{2}y^{2/3} + C_1 &= \frac{3}{2}x^2 + C_2 \\
    y^{2/3} &= x^2 + C \\
    \text{Impose } y(0) &= 0 \\
    0^{2/3} &= 0^2 + C \\ 
    \implies C &= 0 \\
    \tf y^{2/3} &= x^2 \\
    \tf y &= \pm x^3
  \end{align*}
  This is problematic. Our inital value constraint hasn't allowed us to pick one particular solution. 
}
\nt{
  The previous IVP has multiple solutions because $f(x,y)=3xy^{1/3}$ is not differentiable at $y=0$.
}

\ex{}{
  $$
    \dydx = \frac{x-y}{x},\quad y(0)=1\text{ has no solutions:}
  $$
  \begin{align*}
    \dydx &= \frac{x}{x} - \frac{1}{x}y \\
      &= q(x) - p(x)y \\
    \dydx + p(x)y &= 1 \\
    \mu &= \exp\bracks{\int p(x)\d x} \\
      &= \exp\bracks{\int \frac{1}{x}\d x} \\
      &= \exp(\ln(x)) \\
      &= x \\
    \mu\dydx + \mu p(x)y &= \mu \\
    x\dydx + y &= x \\
    \ddx\bracks{xy} &= x \\
    xy &= \int x \d x \\ 
      &= \frac{1}{2}x^2 + C \\
    \Impose y(0) = 1 \\
    \tf 0\cd 1 &= \frac{1}{2}(0)^2 + C \\
    C &= 0 \\
    \tf y(x) &= \frac{1}{2}x
  \end{align*}
  However, our general solution \textbf{does not} satisfy our inital value constraint, $y(0)=\frac{1}{2}(0) = 0 \neq 1$.
} 
\nt{
  Our IVP doesn't have a solution because $f(x,y)=\frac{x-y}{x}$ is not differentiable or continuous around $x=0$.\\

  We're kind of loosley referring to ``existence and uniqueness'' theorems, or Picard-Lindel\"of Theorem, which generally states:
  $$
    \text{The IVP } \dydx = f(x,y)\quad y(x_0)=y_0 
  $$
  has a unique solution around $x_0$ if:
  \begin{enumerate}
    \item $f(x,y)$ is continuous around $(x_0,y_0)$
    \item $f(x,y)$ is differentiable with respect to $y$ around $(x_0,y_0)$, ie $\del{f}{y}$ is continuous aorund $(x_0,y_0)$.
  \end{enumerate}
}

\subsection{Existence and Uniqueness}
\thm{}{
  Consider the IVP
  $$
    \dydx = f(x,y),\quad y(x_0)=y_0
  $$
  We are concerned with the conditions under which a solution exists and is unique.
  \begin{enumerate}
    \item (existence, Peano's Theorem) If $f(x,y)$ is continuous in some rectangle
    $$
      R = \braces{(x,y)\suchthat \abs{x-x_0}<a,\ \abs{y-y_0}<b}
    $$
    then the IVP has at least one solution.
    \item (uniqueness, Picard's Theorem) If $f_y(x,y):=\del{f}{y}$ is also continuous in $R$ then there is some interval $\abs{x-x_0}\leq h\leq a$ which contains at least one solution.
  \end{enumerate}  
}
This result only tells us that a solution exists or is unique locally. Beyond $R$, we simply don't know.

\ex{}{
  $$
    \dydx = x,\quad y(0)=1
  $$
  \begin{align*}
    f(x,y) &= x \\
    f_y(x,y) &= 0
  \end{align*}
  These functions are both continuous over $\bbr^2$. Therefore there exists a unique solution \\

  $$
    \dydx = 3xy^{1/3},\quad y(0)=0\text{ has more than one solution:}
  $$
  \begin{align*}
    f(x,y) &= 3xy^{1/3} \\
    f_y(x,y) &= xy^{-2/3}
  \end{align*}
  $f(x,y)$ is continuous over $\bbr^2$ so there exists at least one solution. However, $f_y$ has a discontinuity at $y=0$, so there may or may not be unique solutions (remember, its not an iff).

  $$
    \dydx = \frac{x-y}{x},\quad y(0)=1\text{ has no solutions:}
  $$
  \begin{align*}
    f(x,y) &= \frac{x-y}{x} \\
    f_y(x,y) &= -\frac{1}{x}
  \end{align*}
  $f(x,y)$ and $f_y$ both have discontinuities when $x=0$, so we don't know from this test if there are solutions, or if the solution is unique.
}

\nt{
  These theorems are not if and only if's. They can fail. For example, take the IVP
  $$
    \dydx = \frac{1}{3}x^{-2/3},\quad y(0)=1/
  $$
  We see
  $$
    f(x,y) = \frac{1}{3}x^{-2/3},\qquad f_y(x,y) = 0
  $$
  $f$ has a discontinuity when $x=0$, so the theorem's fail to identify if this IVP has solutions. However, this IVP \textbf{does} have a unique solution,
  $$
    y(x) = x^{1/3} + 1,
  $$
  so we need to be careful we're using these theorems correctly. \textbf{If} $f$ and $f_y$ are continuous in some reigon \textbf{then} there exists a unique solution in that reigon.
}

\ex{}{
  Solve these:
  \begin{enumerate}
    \item $y' = y^{2/3},\quad y(0)=1$
    \begin{align*}
      f(x,y) &= y^{2/3} \\
      f_y(x,y) &= \frac{2}{3}y^{-1/3}
      \intertext{Therefore there exist at least one solution to the IVP.}
      y^{-2/3}y' &= 1 \\
      \int y^{-2/3}\d y &= \int 1\d x \\
      3 y^{1/3} &= x + C \\
      y^{1/3} &= \frac{1}{3}(x+C) \\
      y &= \frac{1}{27}(x+C)^3 \\
      \Impose y(0) = 1
    \end{align*}
    Imposing the IVP and expanding the cubic expression, will reveal 3 values for C, the nicest of which is 3. The one which satisfies our IVP is
    $$
      y(x) = \frac{1}{27}(x+3)^3
    $$
    Even though those other solutions exist, only one satisfies the IVP, hence this solution is unique.
    \item $y' = (3x^2+4x+2)/(2y-2),\quad y(0)=1$
    \begin{align*}
      f(x,y) &= \frac{3x^2 + 4x + 2}{2(y-1)} \\
      \intertext{Because of the discontinuity at $y=1$, our exisitence theorem fails to identify if solutions exist.}
      y' &= \frac{3x^2 + 4x + 2}{2(y-1)} \\
      2(y-1)y' &= 3x^2 + 4x + 2 \\ 
      2\int y-1\d y &= \int 3x^2 + 4x + 2\ \d x \\
      y^2-2y &= x^3 + 2x^2 + 2x + C \\
      y^2-2y+1 &= x^3 + 2x^2 + 2x + C + 1 \\
      (y-1)^2 &= x^3 + 2x^2 + 2x + C + 1 \\
      \Impose y(0) &= 1 \\
      ((1)-1)^2 &= (0)^3 + 2(0)^2 + 2(0) + C + 1 \\
      \iff C &= -1 \\
      \tf (y-1)^2 &= x^3 + 2x^2 + 2x \\
      \tf y(x) &= 1\pm\sqrt{x^3 + 2x^2 + 2x}
    \end{align*}
    The IVP has two solutions.
  \end{enumerate}
}

\subsection{Method of Succesive Approximations}
To start, we note that it is always possible to apply a variable shift and so that the IVP is expressed:
$$
  \dydx = f(x,y),\qquad y(0)=0
$$
\ex{}{
  $$
    y' = 2(x-1)(y-1),y(1)=2
  $$
  \begin{gather*}
    \Let \bar{x} = x-1 \\
    \Let \bar{y} = y-2 \\
    \So \dydx = \dyd[\bar{y}]{\bar{x}} \\
    \implies \dyd[\bar{y}]{\bar{x}} = 2\bar{x}(\bar{y}+1),\ \bar{y}(0)=0
  \end{gather*}
}
Without loss of generality we will consider this problem where the inital point is at the origin. We can restate the previous theorem 1.1.1 as follows
\thm{}{
  If $f$ and $f_y$ are continuous in some rectangle
  $$
    R = \braces{(x,y)\suchthat \abs{x}\leq a,\ \abs{y}\leq b},
  $$
  then there is some interval $\abs{x}\leq h\leq a$ which contains a uniuqe solution $y=\phi(x)$ of the IVP
  $$
    \dydx = f(x,y),\qquad y(0)=0
  $$
}

\subsubsection*{Equivilance with integral equation}
Let $y=\phi(x)$ be the solution to the IVP 
\[
  \dydx=f(x,y),\qquad y(0)=0, \tag*{(1)}
\]
and note that the function $F(x) = f(x,\phi(x))$ is a continuous function of $x$ only. We then have
\[
  \phi(x) = \int_{0}^{x} F(t)\d t = \int_{0}^{x} f(t,\phi(t))\d t. \tag*{(2)}
\]
Note that $\phi(0)=0$. This is an example of an \textit{integral equation}. Conversely, let $\phi(x)$ satisfy the integral equation (2). By the Fundamental Theorem of Integral Calculus, $\phi'(x) = f (x, \phi(x))$, which implies that $y = \phi(x)$ is a solution of the IVP (1). In other words, the IVP (1) and the integral equation (2) are equivalent, meaning that a solution of one is a solution of the other. Herein we work with (2).

\subsubsection*{Method of succesive approximations}
The goal of the approach is to generate a sequence of functions ${\phi_0, \phi_1, \dots, \phi_n, \dots}$. Starting with the initial function $\phi_0(x) = 0$ (satisfying the initial condition of (1)), the sequence is generated iteratively by
\[
  \phi_{n+1}(x) = \int_{0}^{x} f(t, \phi_n(t))\d t. \tag*{(3)}
\]
Note that each $\phi_n$ satisfies $\phi_n(0) = 0$, but generally not the integral equation (2) itself. However, if there is a $k$, such that $\phi_{k+1}(x) = \phi_k(x)$, then $\phi_k(x)$ is a solution of the integral equation (2) and hence the IVP (1). Generally this does not occur, but we may instead consider \textit{limit functions}. \\

There are 4 key points to consider:
\begin{enumerate}
  \item Do all members of the sequence exist?
  \item Does the sequence converge to a limit function $\phi$?
  \item What are the properties of $\phi$?
  \item If $\phi$ satisfies the IVP (1), are there other solutions?
\end{enumerate}

\ex{}{
  $$
    y' = 2x(y+1),\qquad y(0) = 0
  $$
  \begin{gather*}
    \phi_0(x) = 0,\qquad f(x,y) = 2x(y+1) \\
    \phi_1(x) = \int_{0}^{x} f(t, \phi_0(t))\d t = \int_{0}^{x} f(t, 0)\d t = \int_{0}^{x} 2t(0+1) \d t = t^2\Big|_{0}^{x} = x^2 \\
    \phi_2(x) = \int_{0}^{x} f(t, \phi_1(t))\d t = \int_{0}^{x} f(t, t^2)\d t = \int_{0}^{x} 2t(t^2+1) \d t = \int_{0}^{x} 2t^3+2t\ \d t = \frac{1}{2}t^4 + t^2\Big|_{0}^{x} = \frac{1}{2}x^4 + x^2
    \intertext{Similarly,}
    \phi_3(x) = x^2 + \frac{1}{2}x^4 + \frac{1}{6}x^6 \\
    \phi_4(x) = x^2 + \frac{1}{2}x^4 + \frac{1}{6}x^6 + \frac{1}{24}x^8 \\
    \intertext{\Propo}
    \phi_n(x) = \sum_{i=1}^{n} \frac{1}{i!}x^{2i}
    \intertext{\Proof By induction: True for $n=1$. Suppose True for $n=k$.}
    \Then \phi_{k+1} (x) = \int_{0}^{x} f(t, \phi_k(t))\d t = \int_{0}^{x} 2t\bracks{1+\sum_{i=1}^{k} \frac{1}{i!}t^{2i}} \d t = \int_{0}^{x} \bracks{2t+\sum_{i=1}^{k} \frac{2}{i!}t^{2i+1}} \d t = t^2 + \sum_{i=2}^{k+1} \frac{1}{i!}t^{2i}\Bigg|_{0}^{x} \\
    \tf\phi_{k+1} = x^2 + \sum_{i=2}^{k+1} \frac{1}{i!}x^{2i} = \sum_{i=1}^{k+1} \frac{1}{i!}x^{2i} \\ 
    \intertext{So the proposition is true $\forall n\in\bbn$.  $\QED$}
    \lim_{n\to\infty}\phi_n(x) = \lim_{n\to\infty} \sum_{i=1}^{n} \frac{1}{i!}x^{2i} \text{ exists}\iff \text{the series converges}
    \intertext{Applying the ratio test between two successive terms, $j$ and $j+1$, as $j$ goes to infinty,}
    \lim_{j\to\infty} \abs{\frac{\dfrac{x^{2j+2}}{(j+1)!}}{\dfrac{x^{2j}}{j!}}} = \lim_{j\to\infty} \abs{\frac{x^{2j+2}}{(j+1)!}\cd\frac{j!}{x^{2j}}} = \lim_{j\to\infty} \abs{\frac{x^2}{j+1}} = 0
    \longintertext{Therefore, the series converges! \\ Therefore, the limit, as $n\to\infty$ of $\phi_n$  exists.}
  \end{gather*}
}

\subsection{Exact First Order ODEs}
\dfn{Exact First Order ODE}{
  Recall that if $z=f(x,y)$ is a differentiable function of $x$ and $y$, where $x=g(t)$ and $y=h(t)$ are both differentiable functions of $t$, then $z$ is a differentiable function of $t$, whose derivative is given by the chain rule:
  $$
    \dyd[z]{t} = \del{f}{x}\dyd[x]{t} + \del{f}{y}\dyd[y]{t}
  $$
  Now suppose the equation
  $$
    f(x,y) = C
  $$
  defines $y$ implicitly as a function of $x$. Then $y=y(x)$ can be show to satisfy a first order ODE obtained by using the chain rule above. In this case, $z=f(x,y(x))=C$, so,
  \begin{gather*}
    \dyd[z]{x} = \dd{x}C = 0 = \del{f}{x}\dyd[x]{x} + \del{f}{y}\dydx \\
    \implies f_x + f_yy' = 0
  \end{gather*}
  A first order ODE of form
  $$
    P(x,y) + Q(x,y)\dydx = 0
  $$
  is called exact if there is a function $f(x,y)$ such that 
  $$
    f_x(x,y) = P(x,y)\quad\text{and}\quad f_y(x,y)=Q(x,y).
  $$
  The solution is then given implicitly by the equation
  $$
    f(x,y) = C,
  $$
  where $C$ can usually be determined by some intial condition.
}

\thm{Test for Exactness}{
  Let $\dps{P,Q,\del{P}{y},\del{Q}{x}}$ be continuous over some reigon of interest. Then
  $$
    P(x,y) + Q(x,y)\dydx = 0
  $$
  is an exact ODE if and only if 
  $$
    \del{P}{y} = \del{Q}{x}
  $$
  everywhere in the reigon
}
\begin{proof}
  1. Prove: ODE is exact $\dps{\implies\del{P}{y} = \del{Q}{x}}$. \\
  Recall Clairout's Theorem,
  \begin{gather*}
    \del{^2f}{x\partial y} = \del{^2f}{y\partial x} \text{ if both $f_{xy}$ and $f_{yx}$ are continuous in the reigon.} \\
    \Suppose \text{ODE is exact}\implies \exists f(x,y): \del{f}{x} = P(x,y), \del{f}{y} = Q(x,y) \\
    \del{P}{y} = \del{^2 f}{x\partial y} = \del{^2 f}{y\partial x} = \del{Q}{x},\text{ by Clairout's Theorem.}
  \end{gather*}
  2. Prove: $\dps{\del{P}{y} = \del{Q}{x}\implies}$ ODE is exact. \\
  \begin{gather*}
    \intertext{$\dps{\Suppose \del{P}{y} = \del{Q}{x}}$. We seek a function $f$ such that $f_x=P, f_y=Q$.}
    \Take f(x,y) = \int_{x_0}^x P(x',y)\d x' + \int_{y_0}^y Q(x_0, y') + C \\
    f_x(x,y) = \del{}{x}\bracks{\int_{x_0}^x P(x',y)\d x' + \cancel{\int_{y_0}^y Q(x_0, y')\d y'}} = P(x,y) \\
    f_y(x,y) = \del{}{y}\bracks{\cancel{\int_{x_0}^x P(x',y)\d x'} + \int_{y_0}^y Q(x_0, y')\d y'} = Q(x,y) \\
  \end{gather*}
  Therefore $\dps{P(x,y) + Q(x,y)\dydx = 0}$ is an exact ODE $\dps{\iff\del{P}{y} = \del{Q}{x}}$ everywhere in the reigon.
\end{proof}

\ex{}{
  $$
    \text{Solve the ODE } 2x+e^y + xe^yy' = 0
  $$
  \begin{align*}
    P(x,y) &= 2x+e^y \\
    \del{P}{y} &= e^y \\
    Q(x,y) &= xe^y \\
    \del{Q}{x} &= e^y \\
    \del{P}{y} = \del{Q}{x} &\Rightarrow \text{ODE is exact} \\
    \tf\exists f(x,y): f_x(x,y) &= P = 2x+e^y \\
    \text{and } f_y(x,y) &= Q = xe^y \\
    \implies f = \int P \d x &= \int 2x+e^y\d x \\
      &= x^2 + xe^y + g(y) \\
    \implies f_y(x,y) = xe^y &= \del{}{y}\bracks{x^2 + xe^y + g(y)} \\
     xe^y &= xe^y + \dyd[g]{y} \\
    \implies \dyd[g]{y} &= 0 \\
    \tf f(x,y) &= x^2 + xe^y + C
    \intertext{All solutions to ODE: $f(x,y)=k$.}
    \iff x^2 + xe^y &= k' \tag*{$(k'=k-C)$} \\
    \iff y &= \ln\bracks{\frac{k' - x^2}{x}}
  \end{align*}
}


\section{Lecture 2}
\subsection{Almost Exact ODEs and Integrating Factors}
Suppose we have
$$
  P(x,y) + Q(x,y)\dydx = 0
$$
and
$$
  \del{P}{y} \neq \del{Q}{x}.
$$
This is not an exact ODE, but can we do anything with it anyway? Let's consider an ``integrating factor'' (not to be confused with integrating factors used when solving linear ODEs). \\

The general idea though, is to multiple the ODE by some function, $h(x,y)$ such that the resulting ODE
$$
  h(x,y)P(x,y) + h(x,y)Q(x,y)\dydx = 0
$$
\textit{is} exact. We know that this new ODE is exact if and only if 
$$
  \del{}{y}(hP) = \del{}{x}(hQ)
$$
Let's find a $h$ which accomplishes this:
\begin{gather*}
  \del{}{y}(hP) = \del{}{x}(hQ) \\
  h_yP + hP_y = h_xQ + hQ_x \iff h_yP - h_xQ + h(P_y - Q_x) = 0 
\end{gather*}
Solving for $h$ in general requires us to solve this first order partial differential equation, which is very nasty and also outside the scope of this course. \\

However, we can consider a simplier case, where $h$ is a function of one of the varaibles $x$ or $y$. Let's try $h = h(x)$:
\begin{gather*}
  \dyd[h]{x} = h(x)\frac{P_y-Q_x}{Q} = h\hat{f}
  \intertext{Suppose $\hat{f}$ is a function of one variable, $x$. Then we are left with a first order seperable ODE which we can solve. Once we've solved for $h$, we can find $f(x,y)$ that solves the exact ODE we wanted to solve.}
\end{gather*}
This is not a great technique, it often doesn't work and requires a lot of trial an error. For example, if $h=h(x)$ didn't yield an approriate $f(x,y)$, we could try $h=h(y)$ or $h=h(x)+h(y)$, which would also give us a seperable ODE to solve. If this technique does work, it hints to some underlying symmetry in the differential system we're solving.

\ex{}{
  $$
    \text{Solve}\ (3xy+y^2) + (x^2+xy)\dydx = 0
  $$
  \begin{gather*}
    P(x,y) = 3xy + y^2\qquad Q(x,y) = x^2 + xy \\
    \del{P}{y} = 3x + 2y \neq 2x + y = \del{Q}{x}
    \intertext{So, this ODE is not exact. Can we multiply through by some integrating factor, $h$?}
    \Take h = h(x) \neq 0 \\
    h(3xy+y^2) + h(x^2+xy)\dydx = \hat{p} + \hat{q}\dydx = 0 \\
    \intertext{is exact}
    \iff \del{\hat{p}}{y} = \del{\hat{q}}{x} \iff h(3x + 2y) = h_x(x^2+xy) + h(2x+y) \iff h(x + y) = h_xx(x+y)
    \intertext{Supposing that $x+y\neq0$, we can simplify and find}
    h = h'x
    \intertext{Supposing that $x\neq0$, we can see}
    h' = \frac{1}{x}h
    \intertext{This is a seperable first order ODE we can simply solve,}
    \int \frac{1}{h} \dyd[h]{x}\d x = \int \frac{1}{x}\d x \iff \int \frac{1}{h}\d h = \int \frac{1}{x}\d x \iff \ln\abs{h} = \ln\abs{x} + \hat{\alpha} \iff h(x) = \alpha x,\ \alpha = \exp(\hat{\alpha}).
    \intertext{We're free to choose $\alpha>0$, so we'll take $\alpha=1$ for simplicity, and then multiple our original ODE by our integrating factor $h=h(x)=x$. Check:}
    h(3xy+y^2) + h(x^2+xy)\dydx = 0 \iff x(3xy+y^2) + x(x^2+xy)\dydx = 0 \\ 
    \iff (3x^2y+xy^2) + (x^3+x^2y)\dydx = 0,\ P(x,y) = 3x^2y+xy^2,\ Q(x,y) = x^3+x^2y \\
    \del{P}{y} = 3x^2+2xy = 3x^2+2xy = \del{Q}{x}
    \intertext{So this ODE is exact. Therefore, there exists some functoin, $f(x,y)$ such that $f_x=P$ and $f_y=Q$}
    \Take f(x,y) = x^3y + \frac{1}{2}x^2y^2 \implies f_x =3x^2y + xy^2 = P,\ f_y = x^3 + x^2y
    \intertext{Therefore, the solution to our ODE is}
    f(x,y) = K \iff x^3y + \frac{1}{2}x^2y^2 = K \iff \frac{1}{2}x^2y^2 + x^3y - K = 0 \iff y = \frac{-x^2 \pm \sqrt{x^3 + 2K}}{x}
    \intertext{Purely for fun, we're going to apply an inital condition, $y(1)=0$}
    \Then 0 = \frac{-1 \pm \sqrt{1 + 2K}}{1} \iff 0 = K \text{ and we choose the positive branch}
    \intertext{So our final solution is} 
    y(x) = \frac{x^2 + \sqrt{x^3}}{x} = \sqrt{x}-x
  \end{gather*}
}

\subsection{Hyperbolic Functions}
\dfn{Hyperbolic Functions}{
  \begin{align*}
    \cosh(x) &= \frac{e^x + e^{-x}}{2}, \\
    \sinh(x) &= \frac{e^x - e^{-x}}{2}, \\
    \tanh(x) &= \frac{\sinh (x)}{\cosh(x)} = \frac{1 - e^{-2x}}{1 + e^{-2x}}, \\
  \end{align*}
}
\cor{Hyperbolic-Pythagorean Identity}{
  $$
    \cosh^2(x) - \sinh^2(x) = 1
  $$
}
This follows from direct calculation. \\

Note that the Pythagorean identity $\cos^2x + \sin^2x = 1$ allows us to paramaterise the unit circle, namely by setting $x(t)=\cos t,\ y(t)=\sin t$, which gives us the equation of a unit circle, $\cos^2t+\sin^2t = x^2 + y^2 = 1$. \\

If instead, we set $x(t)=\cosh t,\ y(t)=\sinh t$, we can see
$$
  \cosh^2t - \sinh^2t = x^2 - y^2 = 1
$$
which is the equation for a hyperbola. \\
Also following from direct calculation, similar to their trigonometric counterparts, the hyperbolic functions satisfy
\begin{gather*}
  \ddx\cosh x = \frac{e^x - e^{-x}}{2} = \sinh x,\\ 
  \ddx\sinh x = \frac{e^x + e^{-x}}{2} = \cosh x
\end{gather*}
Note that $\cosh(0)=1,\ \cosh(x)\geq1$ and $\cosh(x)$ is an even function ($\cosh(-x) = \cosh(x)$); $\sinh(0)=0,\ \sinh(x)$ is an odd function $\sinh(-x) = -\sinh(x)$.

\ex{}{
  Prove that:
  \begin{enumerate}
    \item $\cosh^2x = \frac{1}{2}(\cosh(2x)+1)$
    \begin{align*}
      \cosh^2x &= \bracks{\frac{e^{x}+e^{-x}}{2}}^2 \\
        &= \frac{e^{2x} + 2e^{0} + e^{-2x}}{4} \\
        &= \frac{1}{2}\cd\frac{e^{2x} + e^{-2x} + 2}{2} \\
        &= \frac{1}{2}(\cosh 2x + 1)
    \end{align*}
    \item $\sinh^2x = \frac{1}{2}(\cosh(2x)-1)$
    \begin{align*}
      \sinh^2x &= \bracks{\frac{e^x - e^{-x}}{2}}^2 \\
        &= \frac{e^{2x} - 2e^{0} + e^{-2x}}{4} \\
        &= \frac{1}{2}\cd\frac{e^{2x} + e^{-2x}  - 2}{2} \\
        &= \frac{1}{2}(\cosh2x - 1)
    \end{align*}
    \item $\sinh(2x) = 2\coshx\sinhx$
    \begin{gather*}
      \sinh2x = \frac{e^{2x} - e^{-2x}}{2} = \frac{(e^x + e^{-x})(e^x - e^{-x})}{2}\cd\frac{2}{2} = 2\cd\frac{e^x + e^{-x}}{2}\cd\frac{e^x - e^{-x}}{2} = 2\coshx\sinhx
    \end{gather*}
  \end{enumerate}
}

\begin{center}
  \begin{tikzpicture}
    \begin{axis}[
        axis lines=middle,
        samples=20,
        domain=-2.5:2.5,
        % enlargelimits=true,
        grid=both,
        grid style={dotted,gray},
        minor tick num=1,
        legend pos=south east,
        xmin=-2.5, xmax=2.5,
    ]
        \addplot[blue, thick] {cosh(x)};
        \addplot[red, thick] {sinh(x)};
        \addplot[darkgreen, thick] {tanh(x)};
        \addlegendentry{$\cosh(x)$};
        \addlegendentry{$\sinh(x)$};
        \addlegendentry{$\tanh(x)$};
    \end{axis}
  \end{tikzpicture}
\end{center}

Looking at the plots of the functions, we can deduce that
\begin{align*}
  \dom\cosh x &= \bbr       & \dom\sinh x &= \bbr & \dom\tanh x &= \bbr \\
  \ran\cosh x &= [1,\infty) & \ran\sinh x &= \bbr & \ran\tanh x &= (-1,1)
\end{align*}

\dfn{Reciprocal Hyperbolic Functions}{
  \begin{align*}
    \coth(x) &= \frac{1}{\tanh(x)} = \frac{\cosh (x)}{\sinh(x)} = \frac{1 + e^{-2x}}{1 - e^{-2x}} \\
    \sech(x) &= \frac{1}{\cosh(x)} = \frac{2}{e^{x} - e^{-x}} \\
    \csch(x) &= \frac{1}{\sech(x)} = \frac{2}{e^{x} + e^{-x}} \\
  \end{align*}
}

\subsection{Inverse Hyperbolic Functions}
\dfn{Inverse Hyperbolic Functions}{
  If $f$ is \dots $f\inv$ is denoted \dots:
  \begin{align*}
    \cosh(x) && \arcosh(x) \\
    \sinh(x) && \arsinh(x) \\
    \tanh(x) && \artanh(x)
  \end{align*}
}
\begin{center}
  \begin{tikzpicture}
    \begin{axis}[
        axis lines=middle,
        samples=20,
        domain=-2.5:2.5,
        % enlargelimits=true,
        grid=both,
        grid style={dotted,gray},
        minor tick num=1,
        legend pos=south east,
        xmin=-2.5, xmax=2.5,
    ]
        \addplot[blue, thick, domain=1:2.5] {ln(x + sqrt(x^2 - 1))};
        \addplot[red, thick] {ln(x + sqrt(x^2 + 1))};
        \addplot[darkgreen, thick, domain=-0.99:0.99] {0.5*ln((1+x)/(1-x))};
        \addlegendentry{$\arcosh(x)$};
        \addlegendentry{$\arsinh(x)$};
        \addlegendentry{$\artanh(x)$};
    \end{axis}
  \end{tikzpicture}
\end{center}
\begin{align*}
  \dom\arcosh x &= [1,\infty)       & \dom\arsinh x &= \bbr & \dom\artanh x &= (-1,1) \\
  \ran\arcosh x &= [0,\infty) & \ran\arsinh x &= \bbr & \ran\artanh x &= \bbr
\end{align*}

We have the following:
\begin{align*}
  \int\frac{\d x}{\sqrt{1 + x^2}} &= \arsinh x + C \\
  \int\frac{\d x}{\sqrt{1 - x^2}} &= \arcosh x + C,\ x > 1 
\end{align*}

\ex{}{
  Show $\dps{\ddx(\arsinhx)=\frac{1}{\sqrt{1+x^2}}}$.
  \begin{align*}
    \arsinhx &= y(x) \\
    x &= \sinh y \\
    \iff\ddx(x) &= \ddx(\sinh y) \\
    \iff 1 &= \dydx \cd \cosh y \\
    \iff \dydx &= \frac{1}{\cosh y} \\
      &= \frac{1}{\cosh(\arsinhx)} \\
      &= \frac{1}{\sqrt{\cosh^2(\arsinhx)}} \\
      &= \frac{1}{\sqrt{1 + \sinh^2(\arsinhx)}} \\
      &= \frac{1}{\sqrt{1 + \sinh(\arsinhx)\sinh(\arsinhx)}} \\
      &= \frac{1}{\sqrt{1 + x\cd x}} \\
      &= \frac{1}{\sqrt{1 + x^2}}
  \end{align*}

  Show $\dps{\ddx(\arcoshx)=\frac{1}{\sqrt{x^2 - 1}}}$.
  \begin{align*}
    \arcoshx &= y(x) \\
    x &= \cosh y \\
    \iff\ddx(x) &= \ddx(\cosh y) \\
    \iff 1 &= \dydx \cd \sinh y \\
    \iff \dydx &= \frac{1}{\sinh y} \\
      &= \frac{1}{\sinh(\arcoshx)} \\
      &= \frac{1}{\sqrt{\sinh^2(\arcoshx)}} \\
      &= \frac{1}{\sqrt{\cosh^2(\arcoshx) - 1}} \\
      &= \frac{1}{\sqrt{\cosh(\arcoshx)\cosh(\arcoshx) - 1}} \\
      &= \frac{1}{\sqrt{x\cd x - 1}} \\
      &= \frac{1}{\sqrt{x^2 - 1}}
  \end{align*}
}

\ex{}{
  Evaluate $\dps{\int\frac{\d x}{\sqrt{1+x^2}}}$
  \begin{align*}
    1 + \sinh^2 t &= \cosh^2 t \\
    \Let x &= \sinh t \\
    \implies\dyd[x]{t} = \cosh t &\Rightarrow \d x = \cosh t\ \d t \\
    \tf\int\frac{\d x}{\sqrt{1+x^2}} &= \int\frac{\cosh t}{\sqrt{1+\sinh^2t}}\d t \\
      &= \int\frac{\cosh t}{\sqrt{\cosh^2 t}}\d t \\
      &= \int\frac{\cosh t}{\cosh t}\d t \\
      &= \int \d t \\
      &= t + C\\
      &= \arsinhx + C
  \end{align*}

  Evaluate $\dps{\int\frac{\d x}{\sqrt{x^2-1}}}$
  \begin{align*}
    \cosh^2 t - 1 &= \sinh^2 t \\
    \Let x &= \cosh t \\
    \implies\dyd[x]{t} = \sinh t &\Rightarrow \d x = \sinh t\ \d t \\
    \tf\int\frac{\d x}{\sqrt{x^2-1}} &= \int\frac{\sinh t}{\sqrt{\cosh^2t - 1}}\d t \\
      &= \int\frac{\sinh t}{\sqrt{\sinh^2 t}}\d t \\
      &= \int\frac{\sinh t}{\sinh t}\d t \\
      &= \int \d t \\
      &= t + C\\
      &= \arcoshx + C,\ x\geq 1
  \end{align*}
}

\ex{}{
  Show that $\dps{\ddx(\artanhx) = \frac{1}{1 - x^2}}$
  \begin{align*}
    y &= \artanhx \\
    \tanh y &= \tanh\artanhx \\
    \ddx(\tanh y) &= \ddx(x) \\
    \dydx(1 - \tanh^2 y) &= 1 \\
    \dydx &= \frac{1}{1 - \tanh^2 y} \\
    \dydx &= \frac{1}{1 - \tanh^2 \artanhx} \\
      &= \frac{1}{1 - x^2}
  \end{align*}
}

Using partial fractions, we also find that
$$
  \int\frac{\d x}{1 - x^2} = \frac{1}{2}\ln\bracks{\frac{1 + x}{1 - x}} + C
$$
In fact, we have the following identities
\begin{gather*}
  \artanhx = \frac{1}{2}\ln\bracks{\frac{1+x}{1-x}} \\
  \arsinhx = \ln\bracks{x+\sqrt{x^2+1}} \\
  \arcoshx = \ln\bracks{x+\sqrt{x^2-1}} 
\end{gather*}

\ex{}{
  Show that $\dps{\arsinhx = \ln\bracks{x + \sqrt{x^2 + 1}}}$
  \begin{align*}
    y &= \arsinhx \\
    \sinh y &= x \\
      &= \frac{e^y - e^{-y}}{2} \\
    2x &= e^y - e^{-y} \\
    \intertext{Let $z=e^y>0$}
    2x &= z - \frac{1}{z} \\
    0 &= z^2 - 2xz - 1 \\
    \tf z &= \frac{2x\pm\sqrt{4x^2-4(1)(-1)}}{2(1)} \\
      &= \frac{2x\pm\sqrt{4x^2+4}}{2} \\
      &= \frac{2x\pm\sqrt{4(x^2+1)}}{2} \\
      &= \frac{2x\pm2\sqrt{x^2+1}}{2} \\
      &= x\pm\sqrt{x^2+1} \\
    \intertext{Since $z>0$, we'll pick the positive branch}
    e^y &= x + \sqrt{x^2+1} \\
    y &= \ln\bracks{x + \sqrt{x^2 + 1}} \\
    \tf\arsinhx &= \ln\bracks{x + \sqrt{x^2 + 1}}
  \end{align*}
}

\subsection{The Cateary Problem}
One of the most famous problems where hyperbolic functions are used is in determining the profile of a heavy chain (of constant density $\rho$) suspended from two points of equal height (known as a catenary curve). To derive the differential equation satisfied by the profile $y(x)$, we look at the forces acting on a small element of arc. \\
Let $T(x)$ be the tensile force in the chain with constant horizontal component $H$ (since the load is not a function of $x$) and vertical component $V(x)$. The vertical components of the tensile force at either end of the arc are $V$ and $V + \delta V$. The mass of the arc will be $\rho(\delta s)$, so that the force due to gravity is $\rho g(\delta s)$ The horizontal equilibrium is the trivial relation $H = H$, whereas the vertical equilibrium is the more informative
$$
  (V + \delta V) = V + \rho g(\delta s).
$$
Divind both sides by $\delta x$ gives
$$
  \frac{\delta V}{\delta x} = \rho g\frac{\delta s}{\delta x}.
$$
From geometry, we also have the approximation
$$
  \frac{\delta y}{\delta x} \approx \frac{V}{H}.
$$
We also have the approximation to the arclength $\delta s$
$$
  (\delta x)^2 \approx (\delta x)^2 + (\delta y)^2 \implies \frac{\delta s}{\delta x} \approx \sqrt{1+\bracks{\frac{\delta y}{\delta x}}^2}.
$$
Finally we take the limit $\delta x\to 0$ so that $\delta y\to 0$ and $\delta s\to 0$ similtaneously. We then have the following equations
\begin{gather*}
  \dyd[V]{x} = \rho g\dyd[s]{x}, \\
  V = H\dydx, \\
  \dyd[s]{x} = \sqrt{1 + \bracks{\dydx}^2}.
\end{gather*}
Putting these equations together, yields the ODE satisfied by the profile of the chain, $y(x)$,
$$
  \dydxsq = \frac{\rho g}{H}\sqrt{1 + \bracks{\dydx}^2  }.
$$

% TODO:
\subsection{Linear Second-Order Non-Homogenous ODEs and the Wronskian}


\section{Lecture 3}
\subsection{Variation of Parameters}
We've seen that for a linear second-order, non-homogenous IVP,
$$
  y'' + p(x)y' + q(x)y = r(x),\quad y(x_0)=y_0
$$
if $p,q,r$ are continous on an open interval $I$, and the inital condition, $x_0\in I$, then there exisits a solution to the IVP. The solution will be a linear combination of the solution in the homogenous case and the particular case, $y(x) = y_H(x) + y_P(x)$. Assuming the homogenous case is a linear combination of linearly independent $y$s, ie $W(y_1, y_2) \neq 0$, we can apply variation of parameters. The process is as follows:
\begin{enumerate}
  \item Solve $y'' + p(x)y' + q(x)y = 0$, and obtain a fundamental set of solutions, $y_1, y_2$. Calculate the Wronskian, $W(y_1,y_2)(x) = W$.
  \item Set $y_P = u(x)y_1(x) + v(x)y_2(x)$ and substitute into the ODE. We also impose the condition, $u'y_1 + v'y_2 = 0$. We can freely impose this condition because we have two functions, $u,v$, and only one equation they must satisfy, the ODE.
  \item We obtain
  $$
    u(x) = -\int \frac{y_2r}{W}\d x,\qquad v(x) = \int \frac{y_1r}{W}\d x.
  $$
\end{enumerate}
This approach is a variation of the reduction of order, which prescribes taking some solution, $y$, of the associated ODE, and using it to find a particular solution.

\ex{}{
  Derivation of $u(x)$ and $v(x)$ of the variation of parameters.
  \begin{gather*}
    y'' + p(x)y' + q(x)y = r(x) \tag{1}\label{eq:LinSecOrdODE}
    \intertext{Suppose we solved the homogenous case, $y''+py'+qy=0$.}
    \implies\exists y_1(x), y_2(x): W(y_1,y_2)(x) \neq 0,\ y_H(x) = Ay_1(x) + By_2(x) \\
    y_P(x) = u(x)y_1(x) + v(x)y_2(x) \tag{2}\label{eq:VarParams} \\
    \tf y_P' = u'y_1 + uy_1' + v'y_2 + vy_2 \\
    \intertext{Impose that $u'y_1 + v'y_2 = 0$, then}
    y_P' = uy_1' + vy_2' \\
    \tf y_P'' = u'y_1' + uy_1'' + v'y_2' + vy_2'' 
    \intertext{We'll now substitute (\ref{eq:VarParams})'s derivatives back into (\ref{eq:LinSecOrdODE}), and find}
    (u'y_1' + uy_1'' + v'y_2' + vy_2'') + p(uy_1' + vy_2') + q(uy_1 + vy_2) = r
    \intertext{Consider $uy_1'' + puy_1' + quy_1$ and $vy_2'' + pvy_2' + qvy_2$, and note that they are solutions to the homogenous case, and are therefore equal to 0. So we can simply cancel them out, and are left with:}
    u'y_1' + v'y_2' = r
    \intertext{In fact, the entire system has been reduced to the system of equations}
    \left\lbrace \begin{array}{l} u'y_1 + v'y_2 = 0 \\ u'y_1' + v'y_2' = r \end{array} \right. \\
    \iff \begin{pmatrix} y_1(x) & y_2(x) \\ y_1'(x) & y_2'(x) \end{pmatrix} \begin{pmatrix} u' \\ v' \end{pmatrix} = \begin{pmatrix} 0 \\ r(x) \end{pmatrix} \\
    \begin{pmatrix} y_1(x) & y_2(x) \\ y_1'(x) & y_2'(x) \end{pmatrix} = \hat{W},\ \det\hat{W} = \det W(y_1,y_2)(x) = W \neq 0 \implies \hat{W}\text{ is invertible.} \\
    \hat{W}\inv = \frac{1}{\det\hat{W}}\begin{pmatrix} y_2'(x) & -y_2(x) \\ -y_1' & y_1(x) \end{pmatrix} \\
    \tf \begin{pmatrix} u ' \\ v' \end{pmatrix} = \hat{W}\inv\begin{pmatrix} 0 \\ r(x) \end{pmatrix} = \frac{1}{\det\hat{W}}\begin{pmatrix} y_2'(x) & -y_2(x) \\ -y_1' & y_1(x) \end{pmatrix}\begin{pmatrix} 0 \\ r(x) \end{pmatrix} \\
    \begin{pmatrix} u ' \\ v' \end{pmatrix} = \frac{1}{W}\begin{pmatrix} -y_2r \\ y_1r \end{pmatrix} \\
    \iff \left\lbrace \begin{array}{l} \dps{u' = \frac{-y_2r}{W}} \\ \dps{v' = \frac{y_1r}{W}}\end{array} \right.
    \iff \left\lbrace \begin{array}{l} \dps{u = -\int\frac{y_2r}{W}\d x} \\ \dps{v = \int\dfrac{y_1r}{W}\d x} \end{array} \right.
  \end{gather*}
}

\ex{}{
  Solve
  $$
    y'' - 4y' + 5y = \frac{2e^{2x}}{\sin x}
  $$
  using variation of parameters.
  \begin{gather*}
    y = y_H + y_P
    \intertext{Let's ansatz that $y_H = e^{\lm x}$}
    \iff \lm^2 - 4 + 5 = 0 \iff \lm_{1,2} = 2\pm i \iff y_H = Ae^{2x}\cos x + Be^{2x}\sin x \\ 
    W = \det W(y_1, y_2)(x) = \det\begin{pmatrix} e^{2x}\cos x & e^{2x}\sin x \\ 2e^{2x}\cosx - e^{2x}\sinx & 2e^{2x}\sinx + e^{2x}\cosx \end{pmatrix} = e^{4x} \neq 0
    \intertext{Find $y_P = uy_1 + vy_2$}
    u(x) = -\int\frac{y_2r}{W}\d x = -\int\frac{e^{2x}\sinx\frac{2e^{2x}} {\sinx}}{e^{4x}}\d x = -2\int1\ \d x = -2x \\
    v(x) = \int\frac{y_1r}{W}\d x = \int\frac{e^{2x}\cosx\frac{2e^{2x}}{\sin x}}{e^{4x}}\d x = 2\int\cotx\ \d x = 2\ln\abs{\sinx} \\
    \implies y_P = 2\ln\abs{\sinx}e^{2x}\sin x - 2xe^{2x}\cos x \\
    \implies y = Ae^{2x}\cos x + Be^{2x}\sin x + 2\ln\abs{\sinx}e^{2x}\sin x - 2xe^{2x}\cos x
  \end{gather*}
}

\ex{}{
  Solve for $y_P$, given
  $$
    y'' - 4y' + 5y = \frac{2e^{2x}}{\sin x}
  $$
  using reduction of order.
  \begin{gather*}
    y = y_H + y_P
    y_P = U(x)y_H = U(x)e^{2x}\sinx \\
    y_P' = U'e^{2x}\sinx + 2Ue^{2x}\sinx + Ue^{2x}\cosx \\
    y_P'' = e^{2x}\bracks{U''\sinx + 2U'(2\sinx + \cosx) + U(3\sinx + 4\cosx)} \\
    \intertext{Plug $y_P$ and its derivatives back into the ODE}
    e^{2x}\bracks{U''\sinx + 2U'(2\sinx + \cosx) + U(3\sinx + 4\cosx)} \\
    - 4e^{2x}(U'\sinx + 2U\sinx + U\cosx) \\
    + 5e^{2x}(U(x)\sinx) \\
    = \frac{2e^{2x}}{\sin x} \\
    e^{2x}\bracks{U''\sinx + 4U'\sinx + 2U'\cosx + 3U\sinx + 4U\cosx} \\
    + e^{2x}(-4U'\sinx - 8U\sinx - 4U\cosx) \\
    + e^{2x}(5U(x)\sinx) \\
    = \frac{2e^{2x}}{\sin x} \\
    U''e^{2x}\sinx + 2U'e^{2x}\cosx = \frac{2e^{2x}}{\sin x} \\
    U''\sin^2x + 2U'\cosx\sinx = 2 \\
    \ddx\bracks{U'\sin^2x} = 2 \\
    \int\ddx\bracks{U'\sin^2x}\d x = \int2\d x \\
    U'\sin^2x = 2x \\
    \tf U' = \frac{2x}{\sin^2x} \\
    \tf U = 2\ln\abs{\sinx} - 2x\frac{\cosx}{\sinx} \\
    \begin{aligned}
      \implies y_P &= e^{2x}\sinx\bracks{2\ln\abs{\sinx} - 2x\frac{\cosx}{\sinx}} \\
        &= 2e^{2x}\sinx\ln\abs{\sinx} - 2xe^{2x}\cosx
    \end{aligned}
  \end{gather*}
  Which is the same answer we got when solving this using variation of parameters.
}

\subsection{Vector Spaces}
\nt{
  $\bbf$ stands for $\bbr$ or $\bbc$. \\
  Thus, if a statement holds for both $\bbr$ and $\bbc$, we say it holds for $\bbf$. \\
  Elements of $\bbf$ called scalars. 
}

\dfn{Vector Space}{
  Let $V$ be a nonempty set on which the operations addition (`+') and scalar mulitplication (`$\cd$') are defined. $V$ is called a vector space over $\bbf$ if the following hold for all $\ut{u},\ut{v},\ut{w}\in V$ and $k,l\in\bbf$:
  \begin{enumerate}[label=\textbf{(V\arabic*)}]
    \item Closure: $\ut{u} + \ut{v}\in V$
    \item Additive Communitativity: $\ut{u} + \ut{v} = \ut{v} + \ut{u}$
    \item Additive Associativity: $\ut{u} + (\ut{v} + \ut{w}) = (\ut {u} + \ut{v}) + \ut{w}$
    \item Additive Identity: $\exists\ut{0}:\ut{u} + \ut{0} = \ut{u}$
    \item Additive Inverse: $\forall\ut{u},\exists(-\ut{u}):\ut{u} + (-\ut{u}) = \ut{0}$
    \item Closure Under Scalar Multiplication: $k\cd\ut{u}\in V$
    \item Multiplicative-Additive Distributivity: $k\cd(\ut{u}+\ut{v}) = k\cd\ut{u} + k\cd\ut{v}$
    \item Additive-Multiplicative Distributivity: $(k + l)\cd\ut{u} = k\cd\ut{u} + l\cd\ut{u}$
    \item Multiplicative-Multiplicative Distributivity: $k\cd(l\cd\ut{u}) = (kl)\cd\ut{u}$
    \item Multiplicative Identity: $1\cd\ut{u} = \ut{u}$
  \end{enumerate}
  Elements of a vector space are called \textit{vectors}
}
To decide if a given nonempty set is a vector space, we suggest following
\begin{enumerate}
  \item Identify what $V$ is, what are its elements?
  \item Identity what $+$ and $\cd$ are.
  \item Verify closure (V1, V6)
  \item Identity identities and inverses (V4, V5, V10)
  \item Verify communitativity, associativity and distributivity axioms (V2, V3, V7, V8, V9)
\end{enumerate}

\ex{}{
  Consider the set of $n{-}$tupples, $\bbf^n$, where $n\in\bbn$. Is $\bbf^n$ a vector space?
  \begin{enumerate}
    \item $V = \braces{\ut{u} = (u_1, u_2, \dots, u_n)\suchthat \forall i\in\bbn,\ u_i\in\bbf}$
    \item $\ut{u} + \ut{v} = \bracks{u_1+v_1, u_2+v_2, \dots, u_n+v_n}$\\
      $k\cd\ut{u} = \bracks{ku_1, ku_2, \dots, ku_n}$
    \item $\ut{u} + \ut{v} = \bracks{u_1+v_1, u_2+v_2, \dots, u_n+v_n}$. $\forall i\in\bbn,\ u_i+v_i\in\bbf$. Therefore, $\forall\ut{u},\ut{v}\in\bbf^n,\ \ut{u}+\ut{v}\in\bbf^n$. \\
      $k\cd\ut{u} = \bracks{ku_1, ku_2, \dots, ku_n}$. $\forall i\in\bbn,\ k\ut{u}_i\in\bbf$. Therefore, $\forall\ut{u}\in\bbf^n,\ k\in\bbf,\ k\cd\ut{u}\in\bbf^n$.
    \item $\ut{0} = (0,0,\dots,0)$, with $n$ entries. \\
      $\forall\ut{u}\in\bbf^n,\ \exists(-\ut{u}) = \bracks{-u_1, -u_2,\dots, -u_n}\in\bbf^n$ \\
      $1\in\bbf,\ 1\cd\ut{u} = 1$
    \item $\ut{u} + \ut{v} = \bracks{u_1 + v_1, \dots, u_n + v_n} = \bracks{v_1 + u_1,\dots, v_n + u_n} = \ut{v}+\ut{u}$ \\
      $\ut{u}+(\ut{v}+\ut{w}) = \bracks{u_1+(v_1+w_1),\dots,u_n+(v_n+w_n)} = \bracks{(u_1+v_1)+w_1,\dots,(u_n+v_n)+w_n} = (\ut{u}+\ut{v})+\ut{w}$ \\
      $k\cd(\ut{u}+\ut{v}) = \bracks{k(u_1+v_1),\dots,k(u_1+v_1)} = \bracks{ku_1 + ku_1,\dots, ku_n+kv_n} = \bracks{ku_1,\dots,ku_n} + \bracks{kv_1,\dots,kv_n} = k\cd\ut{u} + k\cd\ut{v}$ \\
      $(k+l)\cd\ut{u} = \bracks{(k+l)u_1,\dots,(k+l)u_n} = \bracks{ku_1+lu_1,\dots,ku_n+lu_n} = \bracks{ku_1,\dots,ku_n} + \bracks{lu_1,\dots,lu_n} = k\cd\ut{u} + l\cd\ut{u}$ \\
      $k\cd(l\cd\ut{u}) = k\cd\bracks{lu_1,\dots,lu_n} = \bracks{klu_1,\dots,klu_n} = kl\cd\bracks{u_1,\dots,u_n} = (kl)\cd\ut{u}$
  \end{enumerate}
  Therefore $\bbf^n$ is a vector space.
}

\ex{}{
  Consider the set of $m\times n$ matricies with scalar entries, $M_{m\times n}(\bbf)$. Is this a vector space?

  \begin{enumerate}
    \item $V = \braces{\begin{pmatrix}a_{1,1} & a_{1,2} & \dots & a_{1,n} \\ a_{2,1} & a_{2,2}& \dots & a_{2,n} \\ \vdots & & \ddots \\ a_{m,1} & a_{m,2} & \dots & a_{m,n} \end{pmatrix} \suchthat \forall i,j\in\bbn,\ a_{i,j}\in\bbf}$
    \item `+' is matrix addition; entry-wise addition. $\forall A,B\in M_{m\times n}(\bbf),\ \exists C:\ \forall i,j\in\bbn,\ c_{i,j} = a_{i,j} + b_{i,j}$. \\
      `$\cd$' is scalar multiplication. $kA = C$ where each entry of $C$, $c_{i,j}=ka_{i,j}$.
    \item $\forall i,j\in\bbn, i\leq m, j\leq n,\ a_{i,j},b_{i,j}\in\bbf\implies a_{i,j}+b_{i,j}\iff A+B\in M_{m\times n}(\bbf)$ \\
      $\forall k\in\bbf, ka_{i,j}\in\bbf\iff k\cd A\in M_{m\times n}(\bbf)$
    \item There exists a zero matrix, with all 0 entries. \\
      For all matricies, $A$, there exisits a matrix $-A$, such that $-A = \begin{pmatrix}-a_{1,1} & -a_{1,2} & \dots & -a_{1,n} \\ -a_{2,1} & -a_{2,2}& \dots & -a_{2,n} \\ \vdots & & \ddots \\ -a_{m,1} & -a_{m,2} & \dots & -a_{m,n} \end{pmatrix}$ \\
      $1\in\bbf$, $\forall A\in M_{m\times n}(\bbf),\ 1\cd A = A$.
    \item These axioms are essentially extensions of the $n{-}$tupple proofs we just gave, and I am not going to write them all out right now, but rest assured: they hold.
  \end{enumerate}
  Therefore, $M_{m\times n}(\bbf)$ is a vector space.
}

\subsubsection*{Other examples of vector spaces}
\begin{itemize}
  \item The set of continous real-valued functions on $\sqbracks{a,b}$, $C\sqbracks{a,b}$.
  \begin{gather*}
    \sqbracks{a,b}=D\subset\bbr.\ C\sqbracks{a,b} = V = \braces{f:D\to\bbr\suchthat f \text{ is continuous on } D.} \\
    \forall f,g\in C\sqbracks{a,b},\ f(x)+g(x) = (f + g)(x) \\
    \forall k\in\bbr, f\in C\sqbracks{a,b},\ k\cd f(x) = (k\cd f)(x) \\
    \ut{0}(x) = 0;\ (-f)(x) = -f(x)  
  \end{gather*}
  \item The set of polynomials of degree at most $n$, $P_n(\bbf)$
  \begin{gather*}
    P_n(\bbf) = V = \braces{a_0 + a_1x + a_2x^2 + \cdots + a_nx^n\suchthat \forall i \in\bbz_{\geq 0},\ a_i\in\bbf} \\
    \forall p,q\in P_n(\bbf),\ p+q = (p_0+q_0) + (p_1+q_1)x + (p_2+q_2)x^2 + \cdots + (p_n+q_n)x^n \\
    \forall k\in\bbf,\ p\in P_n(\bbf),\ k\cd p = (kp_0) + (kp_1)x + (kp_2)x^2 + \cdots + (kp_n)x^n \\
    \exists\ut{0}\in P_n(\bbf),\ \ut{0} = 0; \forall p\in P_n(\bbf), \exists(-p): p + (-p) = \ut{0}, (-p) = -p
  \end{gather*}
  \item The set of solutions to a homogenous linear ODE. Consider $y'' + p(x)y' + q(x)y = 0, y = y(x)$
  \begin{gather*}
    V = \braces{y \suchthat y \text{ is a solution to the linear homogenous ODE under consideration}} \\
    \forall y_1,y_2\in V,\ y_1 + y_2 = (y_1+y_2)(x) \in V \\
    \forall k\in\bbf, \forall y\in V,\ k\cd y = (k\cd y)(x)\in V \\
    \ut{0}=0\in V; \forall y\in V,\ -y = (-1\cd y)(x)\in V 
  \end{gather*}
\end{itemize}

\subsection{Linear Algebra Concepts}
\subsubsection*{Linear combination}
For $\ut{v_1}, \ut{v_2}, \dots, \ut{v_n} \in V$ and $\alpha_1, \alpha_2, \dots, \alpha_n \in \bbf$, we call 
$$
  \alpha_1\ut{v_1} + \alpha_2\ut{v_2} + \dots + \alpha_n\ut{v_n}
$$
a linear combination of the vectors $\ut{v_1}, \ut{v_2}, \dots, \ut{v_n}$.

\subsubsection*{Linear independence}
A non-empty set of vectors $S = \braces{\ut{v_1}, \ut{v_2}, \dots, \ut{v_n}} \subseteq V$ is said to be linearly dependent if there exist scalars $\alpha_1, \alpha_2, \dots, \alpha_n$ not all zero such that
$$
  \alpha_1\ut{v_1} + \alpha_2\ut{v_2} + \dots + \alpha_n\ut{v_n} = \ut{0}.
$$
Otherwise, S is called linearly independent, i.e. S is linearly independent if
$$
  \alpha_1\ut{v_1} + \alpha_2\ut{v_2} + \dots + \alpha_n\ut{v_n} = \ut{0} \implies \alpha_1 = \alpha_2 = \dots = \alpha_n = 0.
$$

\subsubsection*{Subspace}
A subset $W \subseteq V$ is called a subspace of $V$ if $W$ is also a vector space with the same addition and scalar multiplication as $V$. In particular, W is required to close under addition and scalar multiplication.

\subsubsection*{Span}
The span of a non-empty set of vectors $S = \braces{\ut{v_1}, \ut{v_2}, \dots, \ut{v_n}} \subseteq V$ is the set of all linear combinations of vectors in $S$, denoted $\Span S$. The set $\Span S$ is a subspace of $V$.
$$
  \Span S = \Span\braces{\ut{v_1}, \ut{v_2},\dots,\ut{v_n}} = \braces{\alpha_1\ut{v_1} + \alpha_2\ut{v_2} + \dots + \alpha_n\ut{v_n} \suchthat \forall i\in\bbn,\ \alpha_i\in\bbf}
$$
If the span of $S$ is equal to the vector space $V$, then $S$ is said to span $V$.

\subsection{Basis}
Let $\beta = \braces{\ut{v_1}_1, \ut{v_2},\dots, \ut{v_n}}\subseteq V$ be a set of vectors in $V$. $\beta$ is a basis for $V$ If
\begin{enumerate}[label=\textbf{(B\arabic*)}]
  \item $\beta$ is linearly independent
  \item $\beta$ spans $V$. ($\Span\beta = V$)
\end{enumerate} 
Note, however, that the notion of basis is only defined for finite sets. A nonzero vector space is \textbf{finite-dimensional} if it contains a finite number of vectors that form a basis; $n$ is a finite number. If no such set exists, the vector space is called \textbf{infinite-dimensional} \\
Basis enables us to concretely define a sense of dimensionality for a vector space, namely
$$
  n = \dim V = \abs{\Span\beta}.
$$
It is said that $V$ is $n{-}$dimensional, or has $n$ dimensions. \\

An \textbf{ordered basis} for a vector space is a basis endowed with a specific order. For some vector spaces, there is a canonical ordered basis, called a standard basis. For example,
\begin{gather*}
  \text{Standard basis of }\bbr^3: \beta = \braces{\ihat, \jhat, \khat} = \braces{\begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix}, \begin{pmatrix} 0 \\ 1 \\ 0 \end{pmatrix}, \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}} \implies \dim\bbr^3 = 3 \\
  \text{Standard basis of }P_3(\bbr): \beta = \braces{1, x, x^2, x^3} \implies \dim P_3(\bbr) = 4 \\
  \text{Standard basis of }M_{m\times n}(\bbr): \beta = \braces{\begin{array}{lll} \begin{pmatrix} 1 & 0 & 0 \\ 0 & 0 & 0 \end{pmatrix}, & \begin{pmatrix} 0 & 1 & 0 \\ 0 & 0 & 0 \end{pmatrix}, & \begin{pmatrix} 0 & 0 & 1 \\ 0 & 0 & 0 \end{pmatrix}, \\ \begin{pmatrix} 0 & 0 & 0 \\ 1 & 0 & 0 \end{pmatrix}, & \begin{pmatrix} 0 & 0 & 0 \\ 0 & 1 & 0 \end{pmatrix}, & \begin{pmatrix} 0 & 0 & 0 \\ 0 & 0 & 1 \end{pmatrix} \end{array}} \implies \dim M_{m\times n}(\bbr) = 6
\end{gather*}

\chapter{Week 2}
\section{Lecture 4}
\subsection{Decomposition Theory}
Let $\beta$ be a set of vectors in the vector space $V$. Then, $\beta$ is a basis for $V$ if and only if each each vector in $V$ can be expressed as a unique linear combination of vectors in $\beta$.
\mlenma{$\beta$ is a basis of $V$ $\implies$ all $\ut{w}\in V$ is a unique linear combination of $v\in\beta$.}{
  Assume $\beta$ is a basis for $V$. \\
  $\dps{\implies\forall\ut{w}\in V,\ \ut{w} = \sum_{i=1}^{n} \alpha_i\ut{v}_i}$, where each $\alpha_i$ is unique. \\
  Suppose $\ut{w}$ is not a unique linear combination, then it could also be expressed as $\dps{\sum_{i=1}^{n} \beta_i\ut{v}_i}$. \\
  Then, $\dps{\ut{0} = \ut{w} + (-\ut{w}) = \sum_{i=1}^{n} \alpha_i\ut{v}_i - \sum_{i=1}^{n} \beta_i\ut{v}_i = \sum_{i=1}^{n}(\alpha_i-\beta_i)\ut{v}_i}$. \\
  $\implies \alpha_i - \beta_i = 0,\ \forall i \iff \alpha_i = \beta_1,\ \forall i$ \\
  Which is a contradition, because we assumed that $\ut{w}$ did not have a unique linear combination. \\
  Therefore, $\ut{w}$ has a unique linear combination.
}
\mlenma{All $\ut{w}\in V$ is a unique linear combination of $v\in\beta\implies\beta$ is a basis of $V$.}{
  Assume $\forall\ut{w}\in V$ is a unique linear combination of vectors in $\ut{v}_i\in\beta$. \\
  $\implies \beta\setminus V = \Span\braces{\beta}$, by axiom \textbf{B2}. \\
  Since $\ut{w}$ is assumed to be unique, we only have one choice of coeficcents, $\alpha_i$. \\
  $\implies\beta$ satisfies \textbf{B1} \\
  $\implies\beta$ is a basis for $V$.
}
\thm{Decomposition Theory}{
  $$
    \beta\ \text{forms a basis of } V \iff \forall\ut{w}\in V,\ \ut{w} = \sum_{i=1}^{n} \alpha_i\ut{v}_i,\ \forall i,\ut{v}_i\in\beta,\ \text{is unique.}
  $$
}
\nt{Not all bases of $V$ are unique!}

\subsection{Transition Matrix}
Let $\beta$ be an ordered basis for the vector space $V$. For $\ut{u}\in V$ let $a_1,a_2,\dots,a_n$ be the unique scalars such that
$$
  \ut{u} = \sum_{i=1}^{n} a_i\ut{v}_i.
$$
\Defin{Coordinate Vector} The coordinate vector of $\ut{u}$ reletive to $\beta$ is given by
$$
  \sqbracks{\ut{u}}_\beta = \begin{pmatrix} a_1 \\ \vdots \\ a_n \end{pmatrix}
$$
We can denote the $i-$th component as $\dps{\sqbracks{\ut{u}}_\beta^i}$
\dfn{Transition Matrix}{
  Let $\beta'$ be another ordered basis of $V$. The transition matrix From $\beta$ to $\beta'$, denoted by $P_{\beta\to\beta'}$ relates the two coordinate vectors of $\ut{u}$ by
  $$
    \sqbracks{\ut{u}}_{\beta'} = P_{\beta\to\beta'}\sqbracks{\ut{u}}_\beta
  $$
}
If $\beta''$ is yet another ordered basis of $V$ then
$$
  P_{\beta'\to\beta''}P_{\beta\to\beta'} = P_{\beta\to\beta''} \implies P_{\beta\to\beta'}P_{\beta'\to\beta} = P_{\beta\to\beta} = I
$$

\ex{}{
  Consider two ordered bases $\beta=\braces{1,x}$ and $\beta'=\braces{1+x,2x}$ of the vector space $P_1(\bbf)$. $\ut{u} = a + bx$ can be rewritten as
  $$
    a(1+x) + \frac{1}{2}(b-a)(2x),
  $$
  so we have
  $$
    \sqbracks{\ut{u}}_\beta = \begin{pmatrix} a \\ b \end{pmatrix} \qquad\text{and}\qquad \sqbracks{\ut{u}}_{\beta'} = \begin{pmatrix} a \\ \frac{1}{2}(b-a) \end{pmatrix}
  $$
  Therefore the transtion matrix $P_{\beta\to\beta'}$ is given
  $$
    \begin{pmatrix} 1 & 0 \\ \frac{-1}{2} & \frac{1}{2} \end{pmatrix}
  $$
  which satisifies $\sqbracks{\ut{u}}_{\beta'} = P_{\beta\to\beta'}\sqbracks{\ut{u}}_{\beta}$
}
In general, with vector space $V$, with $\dim V = n$, and two bases $\beta = \braces{\ut{u}_1, \ut{u}_2, \dots, \ut{u}_n}$ and $\beta' = \braces{\ut{u}_1', \ut{u}_2', \dots, \ut{u}_n'}$ of $V$,
$$
  P_{\beta\to\beta'} = \bracks{\sqbracks{\ut{u_1}}_{\beta'} \suchthat \sqbracks{\ut{u_2}}_{\beta'} \suchthat ... \suchthat \sqbracks{\ut{u_n}}_{\beta'}}
$$

\subsection{Real Inner Product Spaces}
\subsubsection{Dot Product}
Sometimes called the Euclidean inner product, for two vectors $\ut{u},\ut{v}\in\bbr^n$, the dot product is given by
$$
  \ut{u}\cd\ut{v} = u_1v_1 + u_2v_2 + \dots + u_nv_n = \sum_{i=1}^{n} u_iv_i = \ut{u}^T\ut{v}.
$$
This is a map from $\bbr^n\times\bbr^n\to\bbr$, and the following key properties:
\begin{enumerate}[label=(\roman*)]
  \item Symmetric: $\ut{u}\cd\ut{v} = \ut{v}\cd\ut{v}$
  \item Linear 1: $\bracks{\ut{u}+\ut{v}} + \ut{w} = (\ut{u}\cd\ut{w}) + (\ut{v}\cd\ut{w})$
  \item Linear 2: $(k\ut{u})\cd\ut{v} = k(\ut{u}\cd\ut{v})$
  \item Positive Definite 1: $\forall\ut{u}\in\bbr,\ \ut{u}\cd\ut{u} = \abs{\abs{\ut{u}}}^2 \geq 0$
  \item Positive Definite 2: $\ut{u}\cd\ut{u} = 0 \iff \ut{u} = \ut{0}$.
\end{enumerate}
Let's axiom-itize this!

\subsubsection{Inner Product}
An inner product is a function on a vector space, $V$ that maps two vectors in $V$, say $(\ut{u},\ut{v})$, to a real number, denoted $\brangle{\ut{u},\ut{v}}$, such that, for all $\ut{u},\ut{v},\ut{w}\in V$:
\begin{enumerate}[label=(\textbf{I\arabic*})]
  \item Symmetric: $\brangle{\ut{u},\ut{v}} = \brangle{\ut{v},\ut{v}}$
  \item Linear 1: $\brangle{\ut{u}+\ut{v}, \ut{w}} = \brangle{\ut{u},\ut{w}} + \brangle{\ut{v},\ut{w}}$
  \item Linear 2: $\brangle{k\ut{u},\ut{v}} = k\brangle{\ut{u},\ut{v}}$
  \item Positive Definite 1: $\brangle{\ut{u},\ut{u}} \geq 0$
  \item Positive Definite 2: $\brangle{\ut{u},\ut{u}} = 0 \iff \ut{u} = \ut{0}$.
\end{enumerate}
\nt{
  Complex inner product spaces are beyond the scope of this course
}

\ex{Weighted Dot Product}{
  $\forall \ut{u},\ut{v}\in\bbr^n,$ with scalars $\gamma_i>0,\forall i$,
  $$
    \brangle{\ut{u},\ut{v}} = \gamma_1u_1v_1 + \gamma_2u_2v_2 + \dots + \gamma_iu_iv_i = \sum_{i=1}^{n} \gamma_iu_iv_i
  $$
  It's a dot product, except we can ``weight'' certain components higher then others. A special case of the weighted dot product is $\gamma_i=1$ for all $i$, and this is the familar dot product. \\
  STRESS: $\gamma_i>0 \iff$ (\textbf{I5})
}
\ex{Inner Product Generated by a Matrix}{
  Let $\ut{u},\ut{v}\in\bbr^n,\ A\in M_{n\times n}(\bbr),\ A$ is invertible (therefore $\det A \neq 0$). Define an inner product,
  $$
    \brangle{\ut{u},\ut{v}} = (A\ut{u}) \cd (A\ut{v})
  $$
  Note that: $(A\ut{u}) \cd (A\ut{v}) = (A\ut{u})^TA\ut{v} = \ut{u}^TA^TA\ut{v}$. \\
  Interestingly, the weifhted dot product is just a special case of this kind of inner product, only where $A^TA = A^2 = \diag\bracks{\gamma_1,\gamma_2,\dots,\gamma_n} \iff A = \diag\bracks{\sqrt{\gamma_1}, \sqrt{\gamma_2}, \dots, \sqrt{\gamma_n}}$. \\
  STRESS: We assumed $\det A \neq 0 \implies \det(A^TA) = (\det A)^2 \neq 0$. Again $\det A \neq 0 \iff$ (\textbf{I5}) 
}
\ex{Inner Product on $M_{n\times n}(\bbr)$}{
  Note that $\dim M_{n\times n}(\bbr) = n^2$. We start by noting that the trace of a matrix $\ut{u}\in M_{n\times n}(\bbr)$ is
  $$
    \Tr\ut{u} = \sum_{i=1}^{n} \ut{u}_{i,i}.
  $$
  The inner product is defined as 
  $$
    \brangle{\ut{u},\ut{v}} = \Tr\braces{\ut{u}^T\ut{v}}
  $$
  This is a more interesting example, so We'll prove that it is an inner product. \\
  \Lemma I1: $\brangle{\ut{u},\ut{v}} = \Tr\bracks{\ut{u}^T\ut{v}} = \Tr\bracks{\bracks{\ut{u}^T\ut{v}}^T} = \Tr\bracks{\ut{v}^T\ut{u}} = \brangle{\ut{v},\ut{u}} \QED$ \\
  \Lemma I2, I3: $\brangle{\alpha\ut{u} + \beta\ut{v}, \ut{w}} = \Tr\bracks{(\alpha\ut{u}^T + \beta\ut{v}^T)\ut{w}} = \Tr\bracks{\alpha\ut{u}^T\ut{w} + \beta\ut{v}^T\ut{w}} = \Tr\bracks{\alpha\ut{u}^T\ut{w}} + \Tr\bracks{\beta\ut{v}^T\ut{w}} = \alpha\brangle{\ut{u},\ut{w}} + \beta\brangle{\ut{v},\ut{w}} \QED$ \\
  \Lemma I4, I5: $\brangle{\ut{u},\ut{u}} = \Tr\bracks{\ut{u}^T\ut{u}} = \sum_{i=1}^{n}\sum_{j=1}^{n} (u_{i,j})^2 \geq 0$. It follows that this expression can only equal 0 if and only if $\ut{u}=\ut{0}$. \\
  \Theom{Therefore, this is an inner product} $\QED$
}
\ex{Standard Inner Product on $P_n(\bbr)$}{
  $$
    \brangle{\ut{u}, \ut{v}} = p_0q_0 + p_1q_1 + \dots + p_nq_n
  $$
}
\ex{Evaluation Inner Product on $P_n(\bbr)$}{
  Let $x_0, x_1, \dots, x_n\in\bbr$ all be distinct ($x_i\neq x_j \iff i\neq j$). Then
  $$
    \brangle{\ut{p},\ut{q}} = p(x_0)q(x_0) + p(x_1)q(x_1) + \dots + p(x_n)q(x_n) = \sum_{i=0}^{n} p(x_i)q(x_i)
  $$
}
\ex{Innter Product on $C\sqbracks{a,b}$}{
  Let $\ut{f},\ut{g}\in C\sqbracks{a,b}$.
  $$
    \brangle{\ut{f},\ut{g}} = \int_a^b f(x)g(x)\d x
  $$
}

\section{Lecture 5}
\subsection{Orthognality}
\subsubsection{Norm}
The norm (or magnitude or length) of an element $\ut{v} = \bracks{v_1,\dots,v_n}\in\bbr^n$ is given by the familar expression
$$
  \norm{\ut{v}} = \sqrt{\ut{v}\cd\ut{v}} = \sqrt{v_1^2 + \dots + v_n^2}
$$
This can be rewritten using the real inner product space on $V$ notation we've just developed,
$$
  \norm{\ut{v}} = \sqrt{\brangle{\ut{u}, \ut{u}}}
$$
We could use any inner product space, like the examples we presented in the previous chapter. \\
\Defin{Unit Vector} A vector $\ut{v}\in V$, with $\norm{\ut{v}}=1$ is called a unit vector. \\
\dfn{Distance Function}{
  Inspured by our intution on $\bbr^n$, we can define the distance $d\bracks{\ut{u},\ut{v}}$ between two vectors 
  $$
    d\bracks{\ut{u},\ut{v}} = \norm{\ut{u}-\ut{v}}
  $$
  This distance is symmetric, following from
  $$
    \brangle{\ut{u}-\ut{v}, \ut{v}-\ut{u}} = \brangle{\ut{u},\ut{u}} - \brangle{\ut{u},\ut{v}} - \brangle{\ut{v},\ut{u}} + \brangle{\ut{v},\ut{v}} = \brangle{\ut{v} - \ut{u},\ut{u} - \ut{v}}
  $$
}
Critically, the notions of norm and distance are relative to the inner product itself. \\

\Defin{Orthognal} Two vectors $\ut{u},\ut{v}\in V$ are orthognol iff $\brangle{\ut{u},\ut{v}} = 0$. \\
This generalises to all inner product spaces, and gives us a basis to talk about a general ``angle'' between vectors.

\ex{}{
  Consider $V = P_2(\bbr)$ and $\ut{u} = x,\ \ut{v}=x^2\in V$. Let 
  $$
    \brangle{\ut{u},\ut{v}} = \int_{-1}^{1} u(x)v(x)\d x = \int_{-1}^{1} x^3\d x = 0
  $$
  Therefore, in this inner product space, the vectors are orthognol. But if we changed our inner product space
  $$
    \brangle{\ut{u},\ut{v}} = \int_{0}^{1} u(x)v(x)\d x = \int_{0}^{1} x^3\d x = \frac{1}{4}
  $$
  In this inner product space, the vectors are not orthognol.
}

\subsubsection{Pythagorean Theorem}
Let $V$ be a real product space and let $\ut{u},\ut{v}\in V$ then
$$
  \norm{\ut{u} + \ut{v}}^2 = \norm{\ut{u}}^2 + \norm{\ut{v}}^2 \iff \brangle{\ut{u},\ut{v}} = 0
$$
This very well known facto genralises to all inner product spaces.
\begin{proof}
  $$
    \norm{\ut{u}+\ut{v}}^2 = \brangle{\ut{u}+\ut{v}, \ut{u}+\ut{v}} = \brangle{\ut{u},\ut{u}} + 2\brangle{\ut{u},\ut{v}} + \brangle{\ut{v},\ut{v}} = \norm{\ut{u}}^2 + \norm{\ut{v}}^2 + 2\brangle{\ut{u},\ut{v}}
  $$
  Then
  $$
    \norm{\ut{u}+\ut{v}}^2 = 0 \iff \brangle{\ut{u},\ut{v}} = 0
  $$
\end{proof}

\subsubsection{Cauchy-Schwarz Inequality}
Let $V$ be a real inner product space, and let $\ut{u},\ut{v}\in V$ then
$$
  \abs{\brangle{\ut{u},\ut{v}}} \leq  \norm{\ut{u}}\ \norm{\ut{v}}
$$
This inequality is an equality iff $\ut{u}$ or $\ut{v}$ is a scalar multiple of the other vector
\begin{proof}
  First we'll consider the trivial case. Without loss of generality, suppose that $\ut{u}=\ut{0}$. \hfill\\
  Then $0 = \brangle{\ut{u},\ut{v}} = \norm{\ut{u}}\ \norm{\ut{v}} = 0$ \\

  For the non-trivial case, take $\ut{u},\ut{v}\in V: \ut{u},\ut{v}\neq 0$. \\
  Let $a = \norm{\ut{u}}^2>0,\ b = \brangle{\ut{u},\ut{v}},\ c = \norm{\ut{v}}^2>0$. \\
  Consider $t\in\bbr$ and $(t\ut{u},\ut{v})\in V$.
  \begin{align*}
    \implies 0 \leq \norm{t\ut{u}+\ut{v}}^2 &= \brangle{t\ut{u} + \ut{v}, t\ut{u} + \ut{v}} \\
      &= \norm{u}^2 t^2 + 2\brangle{\ut{u},\ut{v}}t + \norm{v}^2 \\
      &= at^2 + 2bt + c
  \end{align*}
  Consider this as a degree-2 polynomial in $t$, and consider the conditions under which it has real solutions
  $$
    0\leq at^2 + 2bt + c \iff b^2 - 4ac \leq 0 \iff b^2 \leq 4ac \iff \brangle{\ut{u},\ut{v}}^2 \leq \norm{\ut{u}}^2\ \norm{\ut{v}}^2 \iff \brangle{\ut{u},\ut{v}} \leq \norm{\ut{u}}\ \norm{\ut{v}}
  $$
  which is the Cauchy-Schwarz inequality we sought to prove.
\end{proof}

\subsubsection{Triangle Inequality}
Let $V$ be a real inner product space, and let $\ut{u},\ut{v}\in V$. Then
$$
  \norm{\ut{u} + \ut{v}} \leq \norm{\ut{u}} + \norm{\ut{v}}
$$
\begin{proof}
  \begin{align*}
    \norm{\ut{u} + \ut{v}}^2 &= \norm{\ut{u}}^2 + 2\brangle{\ut{u},\ut{v}} + \norm{\ut{v}}^2 \\
      &\leq \norm{\ut{u}}^2 + 2\brangle{\ut{u},\ut{v}} + \norm{\ut{v}}^2 \\
      &\leq \norm{\ut{u}}^2 + 2\norm{\ut{u}}\norm{\ut{v}} + \norm{\ut{v}}^2 \tag*{(\text{C-S Inequality})} \\
    \iff \norm{\ut{u} + \ut{v}} &\leq \norm{\ut{u}} + \norm{\ut{v}}
  \end{align*}
\end{proof}

\subsubsection{Angle Between Two Vectors}
In $\bbr^n$, given a $\cd$ product, $\theta$ between $\ut{u},\ut{v}\in\bbr^n$ is given by
$$
  \theta = \arccos\bracks{\frac{\ut{u}\cd\ut{v}}{\norm{\ut{u}}\ \norm{\ut{v}}}},\ \theta\in\sqbracks{0,\pi}
$$
Using this intution, consider an inner product space $V$, with $\brangle{\, ,\,}$
$$
  \implies \theta = \arccos\bracks{\frac{\brangle{\ut{u},\ut{v}}}{\norm{\ut{u}}\ \norm{\ut{v}}}}
$$
Note that
$$
  \frac{\brangle{\ut{u},\ut{v}}}{\norm{\ut{u}}\ \norm{\ut{v}}} \leq 1 \iff -1 \leq \frac{\brangle{\ut{u},\ut{v}}}{\norm{\ut{u}}\ \norm{\ut{v}}} \leq 1,
$$
which aligns perfectly the $\cos$, whose range is $\sqbracks{-1,1}$ and $\arccos$ whose domain is $\sqbracks{-1,1}$.

\subsubsection{Orthogonal Complement}
Let $U$ be a subset of the real inner product space $V$. The orthogonal complement of $U$, denoted $U^\perp$, is the set of all vectors in $V$ that are orthogonal to every vector in $U$. That is 
$$
  U^\perp = \braces{\ut{v}\in V \suchthat \brangle{\ut{v},\ut{u}} = 0,\ \forall\ut{u}\in U}
$$
This is a vector space with addition and sclar multiplication inherited from $V$.

\ex{}{
  For $A\in M_{m\times n}(\bbr),\ \operatorname{Row}(A)^\perp = \cln(A)$ with respect to the Euclidean inner product.
  $$
    A = \begin{pmatrix}
      a_{1,1} & a_{1,2} & \dots & a_{1,n} \\
      a_{2,1} & a_{2,2} & \dots & a_{2,n} \\
      \vdots  & \vdots  & \ddots & \vdots  \\
      a_{m,1} & a_{m,2} & \dots & a_{m,n} \\
    \end{pmatrix} =
    \begin{pmatrix}
      \ut{r}_1 \\ \ut{r}_2 \\ \vdots \\ \ut{r}_m 
    \end{pmatrix} = 
    \begin{pmatrix}
      \ut{c}_1 & \ut{c}_2 & \dots & \ut{c}_n
    \end{pmatrix}
  $$
  \begin{align*}
    \operatorname{Col}(A) &= \Span\braces{\ut{c}_1,\ut{c}_2,\dots,\ut{c}_m} \\
    \operatorname{Row}(A) &= \Span\braces{\ut{r}_1^T,\ut{r}_2^T,\dots,\ut{r}_n^T}
  \end{align*}
  $$
    \Given \ut{x} = \begin{pmatrix}
      x_1 \\ x_2 \\ \vdots \\ x_n
    \end{pmatrix},\ \Take A\ut{x} = \begin{pmatrix}
      \ut{r}_1 \\ \ut{r}_2 \\ vdots \\ \ut{r}_n
    \end{pmatrix} \begin{pmatrix}
      x_1 \\ x_2 \\ \vdots \\ x_n
    \end{pmatrix} = \begin{pmatrix}
      \ut{r}_1\ut{x} \\ \ut{r}_2\ut{x} \\ \vdots \\ \ut{r}_n\ut{x}
    \end{pmatrix} = \begin{pmatrix}
      \ut{r}_1^T\cd\ut{x} \\ \ut{r}_2^T\cd\ut{x} \\ \vdots \\ \ut{r}_n^T\cd\ut{x}
    \end{pmatrix}
  $$
  This tells us that
  $$
    A\ut{x} = 0 \iff \ut{r}_i^t\cd\ut{x} = 0,\ \forall i=1,2,\dots,n
  $$
  which brings us to the conclusion that 
  $$
    \ut{x}\in\cln(A) \iff x\in\operatorname{Row}(A)^\perp \iff \operatorname{Row}(A)^\perp = \cln(A)
  $$
}

\subsubsection{$U^\perp$ is an example of a subspace}
A nonempty set $W$ of a vector space $V$ is a subspace of $V$ is it si a vector space with the same addition and scalar multiplication as $V$. To verify that a subset is a subspace, one checks the following
\begin{enumerate}[label=(\arabic*)]
  \item $\ut{0} \in W$
  \item $\ut{u} + \ut{u} \in W,\ \forall\ut{u}\in W$
  \item $k\ut{u}\in W,\ \forall\ut{u}\in W,\ \forall k\in\bbf$
\end{enumerate}
Now We can prove that $U^\perp$ is a subspace
% TODO:

\subsection{Setting Up the Gram-Schmidt Process}
\subsubsection{Orthogonal Set}
Let $V$ be a real inner product space, a nonempty set of vectors in $V$ is orthogonal if each vector in the set is orthogonal to all the other vectors in the set. That is, the set $\braces{\ut{v}_1,\ut{v}_1,\dots,\ut{v}_1}\subseteq V$ is orthogonal if
$$
  \brangle{\ut{v}_i,\ut{v}_j} = 0,\quad i\neq j
$$
Let $S$ be a finite set of vectors in $V$ such that $\ut{0}\notin S$ and $\abs{S} < \dim V$. Then
$$
  S\ \text{orthogonal} \implies S\ \text{linearly independent}
$$
\begin{proof}
  Let $S=\braces{\ut{v}_1, \ut{v}_2, \dots, \ut{v}_n}: \brangle{\ut{v}_i, \ut{v}_1} = 0,\ i\neq j,\ \norm{\ut{v}_i}^2 > 0$.
  \begin{gather*}
    \implies \ut{0} = k_1\ut{v}_1 + \dots + k_n\ut{v}_n  \implies \forall \ut{v}_i\in S,\ \brangle{\ut{0}, \ut{v}_i} = 0 = \brangle{k_1\ut{v}_1 + \dots + k_n\ut{v}_n, \ut{v}_i} \\
    \iff \ut{0} = k_1\brangle{\ut{v}_1,\ut{v}_i} + k_2\brangle{\ut{v}_2,\ut{v}_i} + \dots + k_i\brangle{\ut{v}_i,\ut{v}_i} \\
    \iff \ut{0} = k_i\norm{\ut{v}_i}^2 \iff k_i = 0 \iff S\ \text{linearly independent}
  \end{gather*}
\end{proof}

\subsubsection{Orthonormal Basis}
An orthogonal set of vectors in $V$ is called orthonormal if all the vectors in the set are unit vectors, that is a set,
$$
  \braces{\ut{e}_1, \ut{e}_2, \dots, \ut{e}_n} \subset V: \brangle{\ut{e}_i, \ut{e}_j} = \delta_{i,j}
$$
where the Kronecker delta is defined by
$$
  \delta_{i,j} = \left\lbrace \begin{array}{ll}
    0, & i\neq j, \\
    1, & i = j
  \end{array} \right.
$$
\nt{
  $$
    A = \begin{pmatrix}
      \delta_{1,1} & \delta_{1,2} & \dots & \delta_{1,n} \\
      \delta_{2,1} & \delta_{2,2} & \dots & \delta_{2,n} \\
      \vdots & \vdots & \ddots & \vdots \\
      \delta_{m,1} & \delta_{m,2} & \dots & \delta_{m,n} \\
    \end{pmatrix} = \begin{pmatrix}
      1 & 0 & \dots & 0 \\
      0 & 1 & \dots & 0 \\
      \vdots & \vdots & \ddots & \vdots \\
      0 & 0 & \dots & 1 \\
    \end{pmatrix} = I
  $$
}

\ex{}{
  Given $\bbr^n$ endowed with the dot product, the set
  $$
    \braces{\begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix}, \begin{pmatrix} 0 \\ 1 \\ 0 \end{pmatrix}, \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}} \subset \bbr^3
  $$
  is an orthonormal basis, according to $\cd$. \\ \hrule ${}^{}$\\
  The set
  $$
     S =\braces{\begin{pmatrix} \frac{1}{\sqrt{3}} \\ \frac{1}{\sqrt{3}} \\ \frac{1}{\sqrt{3}} \end{pmatrix}, \begin{pmatrix} -\frac{1}{\sqrt{2}} \\ 0 \\ \frac{1}{\sqrt{2}} \end{pmatrix}} \subset \bbr^3
  $$
  is orthonormal but not a basis. \\ \hrule ${}^{}$\\
  The set
  $$
    S \cup \braces{\begin{pmatrix} \frac{1}{\sqrt{6}} \\ \frac{1}{\sqrt{6}} \\ -\frac{2}{\sqrt{6}} \end{pmatrix}} \subset \bbr^3
  $$
  is both orthonormal and forms a basis in $\bbr^3$.
}

\Defin{Orthonormal Basis} An orthonormal basis for $V$ is a basis or $V$ that is also an orthonormal set.

\subsubsection{Decomposition Theorem}
Let $S=\braces{\ut{e}_1, \dots, \ut{e}_n}$ be an orthonormal basis for $V$ and let $\ut{u}\in V$. Then
$$
  \ut{u} = \brangle{\ut{u},\ut{e}_1}^2\ut{e}_1 + \dots + \brangle{\ut{u},\ut{e}_n}^2\ut{e}_n
$$
and
$$
  \norm{\ut{u}}^2 = \brangle{\ut{u},\ut{e}_1}^2 + \dots + \brangle{\ut{u},\ut{e}_n}^2
$$
\nt{
  This is, basically, what we do when we write a vector $\ut{u}=(1,2)\in\bbr^2$ as $\ut{u} = 1\ihat + 2\jhat$ and $\norm{\ut{u}}^2 = a^2 + b^2$.\\

  We've decomposed the vector $\ut{u}$ into scalars multiplied by basis vectors.
}

\begin{proof}
  Suppose $S$ is a basis \\
  (1)
  $$
    \implies \forall \ut{u}\in V,\ \ut{u} = \sum_{i=1}^{n} a_i \ut{e}_i,\ a_i\in\bbf\ \text{is unique.}
  $$
  \begin{align*}
    \implies \brangle{\ut{u},\ut{e}_i} &= \brangle{\sum_{j=1}^{n} a_j \ut{e}_i, \ut{e}_i} \\
      &= \sum_{j=1}^{n} a_j\brangle{\ut{e}_i, \ut{e}_i} \\
      &= \sum_{j=1}^{n} a_j\delta_{j,i} \\
      &= a_j \brangle{\ut{e}_j, \ut{e}_j} \\
      &= a_j \delta_{j,j} \\
      &= a_j \\
    \implies a_i = \brangle{\ut{u}, \ut{e}_i}
  \end{align*}
  (2)
  $$
    \norm{\ut{u}}^2 = \brangle{\ut{u},\ut{u}} = \sum_{i=1}^{n}\sum_{j=1}^{n} \brangle{\ut{u},\ut{e}_j}\brangle{\ut{u},\ut{e}_i}\brangle{\ut{e}_i,\ut{e}_j} = \sum_{i=1}^{n}\brangle{\ut{u},\ut{e}_i}^2
  $$
\end{proof}


\subsubsection{Orthogonal Projection}
Let $U$ be a finite-dimensional subspace of the real inner product space $V$. Then each $\ut{v}\in V$ can be written in a unique way as
$$
  \ut{v} = \ut{u} + \ut{w},\ \ut{u},\ut{w}\in U^\perp
$$
In the proof, we will assume that $U$ has an orthonormal basis $S=\braces{\ut{e}_1,\ut{e}_2,\dots,\ut{e}_k}$
\begin{proof}
  Let $\ut{v}\in V$. $\ut{v}$ can be expressed
  $$
    \ut{v} = \ut{u} + (\ut{v} - \ut{u}) - \ut{u} + \ut{w},\ \forall \ut{u}\in U,\ \ut{w} = \ut{v}-\ut{u}
  $$
  We want to find $\ut{u}:\brangle{\ut{u},\ut{w}} = 0 = \brangle{\ut{u}, \ut{v} - \ut{u}}$. \\
  $\exists S$, an orthonormal basis for $U$.
  $$
    \implies \ut{u} = a_1\ut{e}_1 + a_2\ut{e}_2 + \dots + a_k\ut{e}_k
  $$
  We impose $\ut{w}:\brangle{\ut{e}_i, \ut{w}} = 0$.
  $$
    \implies \ut{w} = \ut{u} - \ut{v} = \ut{v} - \sum_{i=1}^{n}\brangle{\ut{u},\ut{e}_i}\ut{e}_i,\ \brangle{\ut{u},\ut{w}} = 0 \implies \ut{w} \in U^\perp
  $$
\end{proof}

\nt{
  This orthonormal projection is unique.
}
Let $\ut{v} = \ut{u} + \ut{w}$ and $\ut{v} = \ut{u}' + \ut{w}'$, where $\ut{u},\ut{u}'\in U$ and $\ut{w},\ut{w}'\in U^\perp$. \\
$\implies\ut{u} + \ut{w} = \ut{u}' + \ut{w}' \iff U\ni\ut{u} - \ut{u}' = \ut{w}' - \ut{w} \in U^\perp \contra \because U\cap U^\perp = \braces{\ut{0}}$. \\

The vector $\ut{u}\in U$ is called the orthognol projection of $\ut{v}$ onto $U$ and is given by
$$
  \proj_U(\ut{v}) = \brangle{\ut{v},\ut{e}_1}\ut{e}_1 + \brangle{\ut{v},\ut{e}_2}\ut{e}_2 + \dots + \brangle{\ut{v},\ut{e}_k}\ut{e}_k
$$
Likewise, the vector $\ut{w}\in U^\perp$ is called the orthognol projection of $\ut{v}$ onto $U^\perp$ and is given by
$$
  \proj_{U^\perp} = \ut{v} - \proj_U(\ut{v})
$$
We take it for granted here, but it is possible to prove that
$$
  \dim V = \dim U + \dim U^\perp
$$
which is a fact that can be helpful in determining the orthogonal complement of a subspace $U$. Indeed, suppose you have managed to find $\dim V - \dim U$ linearly independent vectors that are orthogonal to $U$. Then these vectors will in fact form a basis for $U^\perp$.

\ex{Orthogonal Projection in $\bbr^3$}{
  Let $\bbr^3$ be endowed with the usual dot product, and let
  $$
  U = \Span\braces{\bracks{0,1,0},\bracks{-\frac{4}{5},0,\frac{3}{5}}},\quad \ut{v} = \bracks{1,1,1}.
  $$
  Find the orthogonal projections of $\ut{v}$ onto $U$ and $U^\perp$.
  \begin{align*}
    \ut{u} &= \proj_U(\ut{v}) \\
    &= \brangle{\ut{v},\ut{e}_1}\ut{e}_1 + \brangle{\ut{v},\ut{e}_2}\ut{e}_2 \\
    &= 1\ut{e}_1 + \bracks{\frac{3}{6}-\frac{4}{5}}\ut{e}_2 \\
    &= \bracks{\frac{4}{25},1,\frac{-3}{25}} \\
    \ut{w} &= \ut{v} - \proj_U(\ut{v}) \\
      &= \ut{v} - \ut{u} \\
      &= \bracks{1,1,1} - \bracks{\frac{4}{25},1,\frac{-3}{25}} \\
      &= \bracks{\frac{21}{25}, 0, \frac{28}{25}}
      \intertext{Now, lets find that third basis vector}
      \brangle{\ut{w},\ut{e}_1} = \brangle{\ut{w},\ut{e}_2} &= 0 \\
      \tf\ut{e}_3 &= \frac{\ut{w}}{\norm{\ut{w}}} \\
      &= \bracks{\frac{3}{5},0,\frac{4}{5}}
      \intertext{And so, we've found that} 
      \mathclap{\dim U + \dim U^\perp = 2 + 1 = 3 = \dim\bbr^3} \\
      \mathclap{\implies\bbr^3 = \Span\braces{\bracks{0,1,0},\bracks{\frac{-4}{5},0,\frac{3}{5}},\bracks{\frac{3}{5},0,\frac{4}{5}}}}
    \end{align*}
    }
    
\section{Lecture 6}
\subsection{Gram-Schmidt Process}
\subsubsection{Construction of Orthonormal Basis}
In the previous example, we can see how we started with a pair of vectors, but were able to algorthmically find a third vector, to span the entire parent-vectorspace.\\
This can be generalised to turn a linearly independent set of vectors into an orthonormal set of vectors with the same span as the original set. Applying the algorithm to a basis thus turns the basis into an orthonormal basis. Hence:
\begin{center}
  \boxed{\text{Every finite-dimensional real inner product space has an orthonormal basis.}}
\end{center}
Let $\braces{\ut{v}_1,\dots,\ut{v}_n}$ be a linearly independent set of vectors in the real inner product space $V$. The corresponding algorithm is called the Gram-Schmidt process

\begin{algorithm}[H]
  \KwIn{A linearly independent set of vectors $\braces{\ut{v}_1,\dots,\ut{v}_n}\subset V$}
  \KwOut{An Orthonormal basis for $V$}
  \SetNoFillComment
  \vspace{3mm}
  \Step{1}{
    $\dps{\ut{w}_1 \gets \ut{v}_1}$\;
    $\dps{W_1 \gets \Span\braces{\ut{w}_1} = \Span\braces{\ut{e}_1}}$\;
    $\dps{\ut{e}_1 \gets \frac{\ut{v}_1}{\norm{\ut{v}_1}},\quad \norm{\ut{e}_1} = 1}$\;
  }
  \Step{2}{
    $\dps{\ut{w}_2 \gets v_2 - \proj_{W_1}(\ut{v}_2)} = \ut{v}_2 - \brangle{\ut{v}_2, \ut{e}_1}\ut{e}_1 = \ut{v}_2 - \frac{\brangle{\ut{v}_2, \ut{w}_1}}{\norm{\ut{w}_1}^2}\ut{w}_1$\;
    $\dps{\implies \brangle{\ut{w}_2,\ut{w}_1}  =0}$\;
    $\dps{W_2 \gets \Span\braces{W_1, W_2} = \Span\braces{\ut{e}_1,\ut{e}_2}}$\;
    $\dps{\ut{e}_2 \gets \frac{\ut{w}_2}{\norm{\ut{w}_2}}}$\;
  }
  \Step{3}{
    $\dps{\ut{w}_3 = \ut{v}_3 - \frac{\brangle{\ut{v}_3,\ut{w}_1}}{\norm{\ut{w}_1}^2}\ut{w}_1 - \frac{\brangle{\ut{v}_3,\ut{w}_2}}{\norm{\ut{w}_2}^2}\ut{w}_2 = \ut{v}_3 - \brangle{\ut{v}_3,\ut{e}_1}\ut{e}_1 - \brangle{\ut{v}_3,\ut{e}_2}\ut{e}_2 }$\;
    $\dps{\implies \brangle{\ut{w}_3, \ut{w}_i} = 0,\ \forall i\in\braces{1,2}}$\;
    $\dps{W_3 \gets \Span\braces{\ut{w}_1,\ut{w}_2,\ut{w}_3}}$\;
    $\dps{\ut{e}_3 \gets \frac{\ut{w}_3}{\norm{\ut{w}}^2}}$\;
  }
  \Step{n}{
    $\dps{\ut{w}_n = \ut{v}_n - \sum_{k=1}^{n-1} \frac{\brangle{\ut{v}_n,\ut{w}_k}}{\norm{\ut{w}_k}^2}\ut{w}_k = \ut{v}_n - \sum_{k=1}^{n-1} \brangle{\ut{v}_1,\ut{e}_k}\ut{e}_k}$\;
    $\dps{\implies \brangle{\ut{w}_n,\ut{w}_i} = 0,\ \forall i\in\braces{1,\dots,n-1}}$\;
    $\dps{W_n \gets \Span\braces{\ut{w}_1,\dots,\ut{w}_n}}$\;
    $\dps{\ut{e}_n \gets \frac{\ut{w}_n}{\norm{\ut{w}_n}^2}}$\;
  }
  \Return $\dps{W_n = \Span\braces{\ut{w}_1,\dots,\ut{w}_n} = \Span\braces{\ut{e}_1,\dots,\ut{e}_n}}$\;
  \caption{Gram-Schmidt Process}
\end{algorithm}

\ex{}{
  Construct an orthonormal basis for $P_1(\bbr)$ with inner product 
  $$
    \brangle{\ut{p},\ut{q}} = \int_{-1}^{1}p(x)q(x)\d x,
  $$ 
  where the basis is 
  $$
    \beta = \braces{\begin{array}{c} 1 + x \\ \ut{v}_1 \end{array}, \begin{array}{c} 1 - 2x \\ \ut{v}_2 \end{array}}.
  $$

  \begin{align*}
    \ut{w}_1 &= \ut{v}_1 = \bracks{1 + x} \\
    \norm{\ut{w}_1} &= \norm{1 + x} = \sqrt{\int_{-1}^{1} p(x)^2 \d x} \\
      &= \sqrt{\int_{-1}^{1} 1 + 2x + x^2 \d x} \\
      &= \sqrt{x + x^2 + \frac{1}{3}x^3\Bigg|_{-1}^{1}} \\
      &= \sqrt{(1) + (1)^2 + \frac{1}{3}(1)^3 - (-1) - (-1)^2 - \frac{1}{3}(-1)^3} \\
      &= \sqrt{1 + 1 + \frac{1}{3} + 1 - 1 + \frac{1}{3}} \\
    \tf\norm{\ut{w}_1} &= \sqrt{\frac{8}{3}} = \frac{2\sqrt{3}}{3} \\
    \ut{w}_2 &= \ut{v}_2 - \frac{\brangle{\ut{v}_2,\ut{w}_1}}{\norm{\ut{w}_1}^2}\ut{w}_1 \\
      &= (1 - 2x) - \frac{3}{8}(1+x)\int_{-1}^{1} (1-2x)(1+x)\d x \\
      &= (1 - 2x) - \frac{3}{8}(1+x)\int_{-1}^{1} 1-x-2x^2\ \d x \\
      &= (1 - 2x) - \frac{3}{8}(1+x)\ x-\frac{1}{2}x^2-\frac{2}{3}x^3\Bigg|_{-1}^{1} \\
      &= (1 - 2x) - \frac{3}{8}(1+x) \bracks{(1) - \frac{1}{2}(1)^2 - \frac{2}{3}(1)^3 - (-1) + \frac{1}{2}(-1)^2 + \frac{2}{3}(-1)^3} \\
      &= (1 - 2x) - \frac{3}{8}(1+x) \bracks{1 - \frac{1}{2} - \frac{2}{3} + 1 + \frac{1}{2} - \frac{2}{3}} \\
      &= (1 - 2x) - \frac{3}{8}(1+x) \bracks{\frac{2}{3}} \\
      &= (1 - 2x) - \frac{1}{4}(1+x) \\
    \tf\ut{w}_2 &= \frac{3}{4}(1-3x) \\
    \norm{\ut{w}_2} &= \sqrt{\bracks{\frac{3}{4}}^2 \int_{-1}^{1} (1-3x)^2\ \d x} \\
      &= \sqrt{\frac{9}{16} \int_{-1}^{1} 1-6x+9x^2\ \d x} \\
      &= \sqrt{\frac{9}{16}\ x-3x^2+3x^3\Bigg|_{-1}^{1}} \\
      &= \sqrt{\frac{9}{16}\bracks{(1) - 3(1)^2 + 3(1)^3 - (-1) + 3(-1)^2 - 3(-1)^3}} \\
    \tf\norm{\ut{w}_2}&= \frac{3\sqrt{2}}{2} \\
    \ut{e}_1 &= \frac{\ut{w}_1}{\norm{\ut{w}_1}} = \frac{\sqrt{3}}{2}(1+x) \\
    \ut{e}_2 &= \frac{\ut{w}_2}{\norm{\ut{w}_2}} = \frac{\sqrt{2}}{4}(1-3x) \\
    \tf P_1(\bbr) &= \Span\braces{\frac{\sqrt{3}}{2}(1+x), \frac{\sqrt{2}}{4}(1-3x)}
  \end{align*}
}

\subsection{Least Sqaures Approximation}
A problem in linear algebra is the following:
\begin{center}
  Given a vector $\ut{v}$ in a real inner product space, $V$,\\ give the best approsimation to $\ut{v}$ in a finite-dimensional subspace $U$ of $V$.
\end{center}
This is called the ``least squares problem.'' \\
By ``best approximation,'' we mean to find a vector in a subspace of minimal distance to a given vector in the ambient vector space. So the answer to the problem is to 
\begin{center}
  find $\ut{u}\in U: d(\ut{u},\ut{v})$ is as small as possible.
\end{center}
That is, find $\ut{u}\in U$ that minimises $\norm{\ut{v}-\ut{u}}$.

\thm{Best Approximation Theorem}{
  If $U$ is a finite-dimensional subspace of a real product space $V$, and $\ut{v}\in V$, then $\proj_U(\ut{v})$ is the best approximation to $\ut{v}$ from $U$, given by 
  $$
    \ut{v} - \proj_U(\ut{v}) \leq \norm{\ut{v}-\ut{u}},\ \forall \ut{u}\in U
  $$
}

\chapter{Week 3}
\section{Lecture 7}
\subsection{Applications of Least Squares Approximation}
\subsubsection{Fitting a curve to data}
Expierments yield data (assuming $x$ is unique and exact).
$$
  \bracks{x_1,y_1}, \bracks{x_2,y_2},\dots, \bracks{x_n,y_n}
$$
which can include some measurement error. Theory may predict a polynomial relation between $x$ and $y$, but experimental data rarely matches theoretical predictions exactly. We seek a least sqaures polynomial function of best fit (a regression).

\subsection{Eigenvalues and Eigenvectors}
\subsubsection{Non-singular matricies}
For $n\times n$ sqaure matricies $A$, we have several 


\end{document}
